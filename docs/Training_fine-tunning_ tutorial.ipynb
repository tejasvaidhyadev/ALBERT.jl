{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALBERT Fine tuning Tutorial\n",
    "In this tutorial, we will be going through usage of SOTA transformers. We will be using ALBERT transformer model for this tutorial. You can check this link to understand more about [ALBERT](https://arxiv.org/abs/1909.11942)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code can be founded in PR [#203](https://github.com/JuliaText/TextAnalysis.jl/pull/203)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the following library for our tutorial\n",
    "- TextAnlaysis.ALBERT\n",
    "- WordTokenizer \n",
    "- Transformers and Flux \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using TextAnalysis\n",
    "using TextAnalysis.ALBERT # it is where our model reside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets checkout the model version avaliable in PretrainedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Any,1}:\n",
       " TextAnalysis.ALBERT.ALBERT_V1\n",
       " TextAnalysis.ALBERT.ALBERT_V2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtypes(ALBERT.PretrainedTransformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check different size model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{String,1}:\n",
       " \"albert_base_v1\"\n",
       " \"albert_large_v1\"\n",
       " \"albert_xlarge_v1\"\n",
       " \"albert_xxlarge_v1\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_version( TextAnalysis.ALBERT.ALBERT_V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving forward let us look at the following basic steps involved in using any transformer,\n",
    "\n",
    " ### For preprocessing\n",
    "- Tokenize the input data and other input details such as Attention Mask for BERT to not ignore the attention on padded sequences.\n",
    "- Convert tokens to input ID sequences.\n",
    "- Pad the IDs to a fixed length.\n",
    "\n",
    "### For modelling\n",
    "- Load the model and feed in the input ID sequence (Do it batch wise suitably based on the memory available).\n",
    "- Get the output of the last hidden layer\n",
    "- Last hidden layer has the sequence representation embedding at 1th index\n",
    "- These embeddings can be used as the inputs for different machine learning or deep learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`WordTokenizer` will handle the Preprocessing part\n",
    "and `TextAnlaysis` will handle Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This program has requested access to the data dependency albert_base_v1.\n",
      "which is not currently installed. It can be installed automatically, and you will not see this message again.\n",
      "\n",
      "sentencepiece albert vocabulary file by google research .\n",
      "Website: https://github.com/google-research/albert\n",
      "Author: Google Research\n",
      "Licence: Apache License 2.0\n",
      "albert base version1 of size ~500mb download.\n",
      "\n",
      "\n",
      "\n",
      "Do you want to download the dataset from https://drive.google.com/uc?export=download&id=1RKggDgmlJrSRsx7Ro2eR2hTNuMmzyUJ7 to \"/home/iamtejas/.julia/datadeps/albert_base_v1\"?\n",
      "[y/n]\n",
      "stdin> y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Downloading\n",
      "│   source = https://doc-00-3g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/h8ejgoll8vvi3skb2dmd7ntrm80frbea/1595875500000/15884229709856900679/*/1RKggDgmlJrSRsx7Ro2eR2hTNuMmzyUJ7?e=download\n",
      "│   dest = /home/iamtejas/.julia/datadeps/albert_base_v1/albert_base_v1.bson\n",
      "│   progress = NaN\n",
      "│   time_taken = 5.01 s\n",
      "│   time_remaining = NaN s\n",
      "│   average_speed = 6.788 MiB/s\n",
      "│   downloaded = 33.981 MiB\n",
      "│   remaining = ∞ B\n",
      "│   total = ∞ B\n",
      "└ @ HTTP /home/iamtejas/.julia/packages/HTTP/BOJmV/src/download.jl:119\n",
      "┌ Info: Downloading\n",
      "│   source = https://doc-00-3g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/h8ejgoll8vvi3skb2dmd7ntrm80frbea/1595875500000/15884229709856900679/*/1RKggDgmlJrSRsx7Ro2eR2hTNuMmzyUJ7?e=download\n",
      "│   dest = /home/iamtejas/.julia/datadeps/albert_base_v1/albert_base_v1.bson\n",
      "│   progress = NaN\n",
      "│   time_taken = 6.58 s\n",
      "│   time_remaining = NaN s\n",
      "│   average_speed = 6.972 MiB/s\n",
      "│   downloaded = 45.903 MiB\n",
      "│   remaining = ∞ B\n",
      "│   total = ∞ B\n",
      "└ @ HTTP /home/iamtejas/.julia/packages/HTTP/BOJmV/src/download.jl:119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3-element Array{Any,1}:\n",
       " CompositeEmbedding(tok = Embed(128), segment = Embed(128), pe = PositionEmbedding(128, max_len=512), postprocessor = Positionwise(LayerNorm(128), Dropout(0.1)))\n",
       " albert(layers=12, head=12, head_size=64, pwffn_size=3072, size=768)\n",
       " (pooler = Dense(768, 768, tanh), masklm = (transform = Chain(Dense(768, 128, gelu), LayerNorm(128)), output_bias = Float32[-5.345022, 2.1769698, -7.144285, -9.102521, -8.083536, 0.56541324, 1.2000155, 1.4699979, 1.5557922, 1.9452884  …  -0.6403663, -0.9401073, -1.0888876, -0.9298268, -0.64744073, -0.47156653, -0.81416136, -0.87479985, -0.8785063, -0.5505797]), nextsentence = Chain(Dense(768, 2), logsoftmax))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = ALBERT.from_pretrained( \"albert_base_v1\") #here we are using version 1 i.e base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using WordTokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get more detail on tokenizer refer the following [blog](https://tejasvaidhyadev.github.io/blog/Hey-Albert) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This program has requested access to the data dependency albert_large_v1_30k-clean.vocab.\n",
      "which is not currently installed. It can be installed automatically, and you will not see this message again.\n",
      "\n",
      "sentencepiece albert vocabulary file by google research .\n",
      "Website: https://github.com/google-research/albert\n",
      "Author: Google Research\n",
      "Licence: Apache License 2.0\n",
      " albert large version1 of size ~800kb download.\n",
      "\n",
      "\n",
      "\n",
      "Do you want to download the dataset from https://raw.githubusercontent.com/tejasvaidhyadev/ALBERT.jl/master/src/Vocabs/albert_large_v1_30k-clean.vocab to \"/home/iamtejas/.julia/datadeps/albert_large_v1_30k-clean.vocab\"?\n",
      "[y/n]\n",
      "stdin> y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Downloading\n",
      "│   source = https://raw.githubusercontent.com/tejasvaidhyadev/ALBERT.jl/master/src/Vocabs/albert_large_v1_30k-clean.vocab\n",
      "│   dest = /home/iamtejas/.julia/datadeps/albert_large_v1_30k-clean.vocab/albert_large_v1_30k-clean.vocab\n",
      "│   progress = 1.0\n",
      "│   time_taken = 0.17 s\n",
      "│   time_remaining = 0.0 s\n",
      "│   average_speed = 3.154 MiB/s\n",
      "│   downloaded = 536.127 KiB\n",
      "│   remaining = 0 bytes\n",
      "│   total = 536.127 KiB\n",
      "└ @ HTTP /home/iamtejas/.julia/packages/HTTP/BOJmV/src/download.jl:119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WordTokenizers.SentencePieceModel(Dict(\"▁shots\" => (-11.2373, 7281),\"▁ordered\" => (-9.84973, 1906),\"▁doubtful\" => (-12.7799, 22569),\"▁glancing\" => (-11.6676, 10426),\"▁disrespect\" => (-13.13, 26682),\"▁without\" => (-8.34227, 367),\"▁pol\" => (-10.7694, 4828),\"chem\" => (-12.3713, 17661),\"▁1947,\" => (-11.7544, 11199),\"▁kw\" => (-10.4402, 3511)…), 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm = load(ALBERT_V1,1) #because we are using base-version1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use DataLoader avaliable in [`Transformers`](https://github.com/chengchingwen/Transformers.jl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using QNLI Dataseet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Channel{String}(sz_max:0,sz_curr:1), Channel{String}(sz_max:0,sz_curr:0), Channel{String}(sz_max:0,sz_curr:0))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Transformers.Datasets\n",
    "using Transformers.Datasets.GLUE\n",
    "using Transformers.Basic\n",
    "task = GLUE.QNLI()\n",
    "datas = dataset(Train, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux: onehotbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Preprocessing function \n",
    "\n",
    "APIs[WIP] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preprocess (generic function with 1 method)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "makesentence(s1, s2) = [\"[CLS]\"; s1; \"[SEP]\"; s2; \"[SEP]\"]\n",
    "function preprocess(training_batch)\n",
    "ids =[]\n",
    "sent = []\n",
    "for i in 1:length(training_batch[1])\n",
    "    sent1 = tokenizer(spm,training_batch[1][i])\n",
    "    sent2 = tokenizer(spm,training_batch[2][i])\n",
    "    id = makesentence(sent1,sent2)\n",
    "    push!(sent, id)\n",
    "    push!(ids,ids_from_tokens(spm,id))\n",
    "end\n",
    "    mask = getmask(convert(Array{Array{String,1},1}, sent)) #better API underprogress\n",
    "\n",
    "E = Flux.batchseq(ids,1)\n",
    "E = Flux.stack(E,1)\n",
    "length(E) #output embedding matrix\n",
    "segment = fill!(similar(E), 1)\n",
    "    for (i, sent) ∈ enumerate(sent)\n",
    "      j = findfirst(isequal(\"[SEP]\"), sent)\n",
    "      if j !== nothing\n",
    "        @view(segment[j+1:end, i]) .= 2\n",
    "      end\n",
    "end\n",
    "data = (tok = E,segment = segment)\n",
    "labels = get_labels(task)\n",
    "label = onehotbatch(training_batch[3], labels)\n",
    "return(data,label,mask)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 2 methods)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux\n",
    "using Flux: gradient\n",
    "import Flux.Optimise: update!\n",
    "\n",
    "clf = Flux.Chain(\n",
    "    Flux.Dropout(0.1),\n",
    "    Flux.Dense(768, length(labels)), Flux.logsoftmax\n",
    ")\n",
    "\n",
    "ps = params(transformer[1])\n",
    "opt = ADAM(1e-4)\n",
    "#define the loss\n",
    "function loss(data, label, mask=nothing)\n",
    "    e = transformer[1](data)\n",
    "    t = transformer[2](e)\n",
    "    l = logcrossentropy( label,\n",
    "         clf(\n",
    "            transformer[3].pooler(\n",
    "                t[:,1,:]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return l\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l = 0.8853355f0\n",
      "l = 0.57006735f0\n",
      "l = 0.9809218f0\n",
      "l = 0.5881124f0\n",
      "l = 0.78463817f0\n",
      "l = 0.76752764f0\n",
      "l = 0.7264092f0\n",
      "l = 0.7885215f0\n",
      "l = 0.5286734f0\n",
      "l = 0.64378977f0\n",
      "l = 0.7589431f0\n",
      "l = 0.87304103f0\n",
      "l = 0.7476368f0\n",
      "l = 0.7716043f0\n",
      "l = 0.6841873f0\n",
      "l = 0.7801976f0\n",
      "l = 0.5601203f0\n",
      "l = 0.6203372f0\n",
      "l = 0.6522941f0\n",
      "l = 0.6564876f0\n"
     ]
    }
   ],
   "source": [
    "for i ∈ 1:20 # iteration of 20 cycles\n",
    "data_batch = get_batch(datas, 4)\n",
    "data_batch, label_batch, mask = preprocess(data_batch)\n",
    "l = loss(data_batch, label_batch, mask)\n",
    "@show l\n",
    "  grad = gradient(()->l, ps)\n",
    "  update!(opt, ps, grad)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
