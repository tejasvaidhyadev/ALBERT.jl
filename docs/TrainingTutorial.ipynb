{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALBERT\n",
    "An upgrade to BERT that advances the state-of-the-art performance on 12 NLP tasks\n",
    " \n",
    "The success of ALBERT demonstrates the importance of identifying the aspects of a model that give rise to powerful contextual representations. By focusing improvement efforts on these aspects of the model architecture, it is possible to greatly improve both the model efficiency and performance on a wide range of NLP tasks\n",
    "\n",
    "## WHY ALBERT?\n",
    "> An ALBERT configuration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster.\n",
    "\n",
    "ALBERT is a “lite” version of Google’s 2018 NLU pretraining method BERT. It has fewer parameter than BERT\n",
    "\n",
    "In this notebook we are going to extract contextualised wordembedding by ALBERT and learing about classifer available for pretraining and finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Julia- Flux ALBERT Model\n",
    "It very easy and similar to any of the other Flux layer for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using TextAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ *ignore all the warning as TextAnalysis is checked out for developement*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using TextAnalysis.ALBERT # it is where our model reside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we are going to use DataDeps for handling download of pretrained model of ALBERT\n",
    "- For now we are directly laoding \n",
    "- other pretrained Weights can be found [here](https://drive.google.com/drive/u/1/folders/1HHTlS_jBYRE4cG0elITEH7fAkiNmrEgz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Any,1}:\n",
       " CompositeEmbedding(tok = Embed(128), segment = Embed(128), pe = PositionEmbedding(128, max_len=512), postprocessor = Positionwise(LayerNorm(128), Dropout(0.1)))\n",
       " TextAnalysis.ALBERT.albert_transformer(Dense(128, 768), TextAnalysis.ALBERT.ALGroup(Stack(Transformer(head=12, head_size=64, pwffn_size=3072, size=768)), Dropout(0.1)), 12, 1, 1)\n",
       " (pooler = Dense(768, 768, tanh), masklm = (transform = Chain(Dense(768, 128, gelu), LayerNorm(128)), output_bias = Float32[-5.345022, 2.1769698, -7.144285, -9.102521, -8.083536, 0.56541324, 1.2000155, 1.4699979, 1.5557922, 1.9452884  …  -0.6403663, -0.9401073, -1.0888876, -0.9298268, -0.64744073, -0.47156653, -0.81416136, -0.87479985, -0.8785063, -0.5505797]), nextsentence = Chain(Dense(768, 2), logsoftmax))"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BSON: @save, @load\n",
    "@load \"/home/iamtejas/Downloads/albert_base_v1.bson.tfbson\" config weights vocab\n",
    "transformer = TextAnalysis.ALBERT.load_pretrainedalbert(config, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Todo \n",
    "better output repesentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using WordTokenizers #we have albert_tokenizer residing in WordTokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demo we are taking only 3 sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{String,1}:\n",
       " \"God is Great! I won a lottery.\"\n",
       " \"If all their conversations in the three months he had been coming to the diner were put together, it was doubtful that they would make a respectable paragraph.\"\n",
       " \"She had the job she had planned for the last three years.\""
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1 = \"God is Great! I won a lottery.\"\n",
    "sample2 = \"If all their conversations in the three months he had been coming to the diner were put together, it was doubtful that they would make a respectable paragraph.\"\n",
    "sample3 = \"She had the job she had planned for the last three years.\"\n",
    "sample = [sample1,sample2,sample3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading of tokenizer form all the available  since we are using base_V1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordTokenizers.Sentencepiecemodel([\"<pad>\", \"<unk>\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"(\", \")\", \"\\\"\", \"-\", \".\"  …  \"_archivist\", \"_obverse\", \"error\", \"_tyrion\", \"_addictive\", \"_veneto\", \"_colloquial\", \"agog\", \"_deficiencies\", \"_eloquent\"], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  -13.5298, -13.5298, -13.5298, -13.5299, -13.5299, -13.53, -13.5313, -13.5318, -13.5323, -13.5323])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm = load(ALBERT_V1,\"albert_base_v1_30k-clean.vocab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32×3 Array{Int64,2}:\n",
       "   14     14    14\n",
       "    2      2     2\n",
       " 5649    411   439\n",
       "   26     66    42\n",
       "   14     67    15\n",
       "    2  13528  1206\n",
       "  100     20    40\n",
       "  722     15    42\n",
       "  188    133  2036\n",
       "   14    819    27\n",
       "    2     25    15\n",
       "  231     42   237\n",
       "   22     75   133\n",
       "    ⋮         \n",
       "    1     16     1\n",
       "    1     33     1\n",
       "    1     24     1\n",
       "    1  22569     1\n",
       "    1     31     1\n",
       "    1     60     1\n",
       "    1     84     1\n",
       "    1    234     1\n",
       "    1     22     1\n",
       "    1  22740     1\n",
       "    1  20600     1\n",
       "    1     10     1"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = ids_from_tokens(spm, tokenizer(spm,sample[1]))\n",
    "s2 = ids_from_tokens(spm, tokenizer(spm,sample[2]))\n",
    "s3 = ids_from_tokens(spm, tokenizer(spm,sample[3]))\n",
    "E = Flux.batchseq([s1,s2,s3],1)\n",
    "E = Flux.stack(E,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32×3 Array{Int64,2}:\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " ⋮     \n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_indices = ones(Int, size(E)...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know input embedding requires both segment and token indices\n",
    "\n",
    "the `embedding` itself handle position and addtion operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128×32×3 Array{Float32,3}:\n",
       "[:, :, 1] =\n",
       "  0.215637     0.974126    0.923233   …  -0.953364   -1.23193    -1.45834\n",
       "  0.382527    -0.440211   -0.493132      -0.450212   -0.497128   -0.664383\n",
       " -1.06308     -0.596149    1.38881       -0.893396   -0.817059   -0.753805\n",
       "  0.0192553   -0.606222   -0.0422194      0.213562    0.366818    0.655723\n",
       " -1.73208     -0.217243    0.329404       0.720658    0.467273    0.371431\n",
       "  1.44428      0.352459   -0.307714   …   0.729       0.681432    0.574053\n",
       " -2.87975      1.02795    -1.55634       -0.0406037  -0.331129   -0.634261\n",
       "  0.888469     2.01966     0.139051      -1.54702    -1.5215     -1.46764\n",
       "  0.423823     0.10119    -1.7348        -1.40969    -0.998497   -0.439296\n",
       "  0.519439     0.50028     1.78553        1.24271     0.801729    0.306854\n",
       " -2.73993     -2.15839    -0.950032   …  -0.726077   -1.00341    -1.15917\n",
       " -0.739749    -0.58749     0.697434       2.10209     1.90288     1.72213\n",
       " -2.06217     -1.16219     1.14314        1.72633     1.4998      1.32549\n",
       "  ⋮                                   ⋱               ⋮          \n",
       " -1.02152     -0.617795   -0.584283      -1.21236    -1.60555    -2.02886\n",
       "  0.590658     0.308233   -2.58624        0.18208    -0.327743   -0.678027\n",
       "  0.565229     0.311107    1.38155       -0.111639    0.160319    0.460262\n",
       "  0.545883    -0.199483   -0.870848      -4.24455    -4.20667    -4.14337\n",
       "  2.15259      0.0522619   1.29933    …  -0.0296104  -0.0392038  -0.0438145\n",
       " -0.433061    -0.985746    0.175664       0.368157    0.616549    0.803371\n",
       "  0.00152145   0.997358    0.50851       -0.440633   -0.85439    -0.964204\n",
       "  2.71532     -0.906451    1.05152       -1.44782    -1.46486    -1.43477\n",
       "  1.82701      1.27142    -0.359613       1.04111     0.849804    0.684699\n",
       " -2.36265     -1.24984     0.74318    …   1.06401     1.02021     1.00714\n",
       " -0.00173678  -1.83543     0.705052      -2.21205    -2.00926    -1.76395\n",
       "  1.65709     -1.17497     1.70831       -1.07015    -1.11008    -0.99317\n",
       "\n",
       "[:, :, 2] =\n",
       "  0.215637     0.974126   -2.42081   …   1.54524    0.237261   -0.911703\n",
       "  0.382527    -0.440211   -0.86025      -0.914859  -0.779649    0.207117\n",
       " -1.06308     -0.596149    2.62228      -1.1039     1.05774    -2.34221\n",
       "  0.0192553   -0.606222    0.414787     -1.73591   -1.3431     -0.440628\n",
       " -1.73208     -0.217243   -0.437847     -0.796834   0.618275    0.38779\n",
       "  1.44428      0.352459    1.23649   …   0.152725   0.955321    0.123936\n",
       " -2.87975      1.02795    -3.16862       1.73907   -2.82513    -0.256108\n",
       "  0.888469     2.01966    -1.2917       -1.07269   -0.537269    0.075918\n",
       "  0.423823     0.10119     2.23237      -1.66915   -1.67471     0.012503\n",
       "  0.519439     0.50028    -0.101554      1.35643    1.18131     0.101195\n",
       " -2.73993     -2.15839    -1.61688   …   0.627917   2.14858    -2.00197\n",
       " -0.739749    -0.58749    -1.4243        0.880429   1.36264     1.42438\n",
       " -2.06217     -1.16219     0.640386      0.935937  -0.365015    2.21704\n",
       "  ⋮                                  ⋱              ⋮          \n",
       " -1.02152     -0.617795   -1.05234      -0.334482  -1.4045      0.210466\n",
       "  0.590658     0.308233    0.821547      1.21049   -1.18044    -1.63386\n",
       "  0.565229     0.311107    1.09554       0.619731  -1.56184     0.857222\n",
       "  0.545883    -0.199483   -0.171578      0.169927   0.552766   -3.30788\n",
       "  2.15259      0.0522619   0.622951  …   1.77554   -2.0677      1.34322\n",
       " -0.433061    -0.985746    0.762181     -1.2847    -0.0238061   0.59986\n",
       "  0.00152145   0.997358   -0.498533      3.38914   -1.50944    -0.203017\n",
       "  2.71532     -0.906451    1.66836      -0.822932  -1.21968    -1.71859\n",
       "  1.82701      1.27142     2.1175        0.95669   -0.652382   -1.59357\n",
       " -2.36265     -1.24984     0.510399  …  -1.01326    0.230851   -0.992337\n",
       " -0.00173678  -1.83543    -1.33135      -0.863463  -0.460078   -2.36823\n",
       "  1.65709     -1.17497    -0.91338      -0.729243  -1.3999      1.49278\n",
       "\n",
       "[:, :, 3] =\n",
       "  0.215637     0.974126   -0.225817   …  -0.953364   -1.23193    -1.45834\n",
       "  0.382527    -0.440211   -1.56297       -0.450212   -0.497128   -0.664383\n",
       " -1.06308     -0.596149    1.76136       -0.893396   -0.817059   -0.753805\n",
       "  0.0192553   -0.606222    0.63075        0.213562    0.366818    0.655723\n",
       " -1.73208     -0.217243   -0.64694        0.720658    0.467273    0.371431\n",
       "  1.44428      0.352459   -1.68997    …   0.729       0.681432    0.574053\n",
       " -2.87975      1.02795    -1.09577       -0.0406037  -0.331129   -0.634261\n",
       "  0.888469     2.01966    -1.05314       -1.54702    -1.5215     -1.46764\n",
       "  0.423823     0.10119    -0.0889101     -1.40969    -0.998497   -0.439296\n",
       "  0.519439     0.50028     1.43852        1.24271     0.801729    0.306854\n",
       " -2.73993     -2.15839    -1.82027    …  -0.726077   -1.00341    -1.15917\n",
       " -0.739749    -0.58749    -0.698298       2.10209     1.90288     1.72213\n",
       " -2.06217     -1.16219    -1.95819        1.72633     1.4998      1.32549\n",
       "  ⋮                                   ⋱               ⋮          \n",
       " -1.02152     -0.617795    1.02958       -1.21236    -1.60555    -2.02886\n",
       "  0.590658     0.308233   -1.30594        0.18208    -0.327743   -0.678027\n",
       "  0.565229     0.311107   -0.594398      -0.111639    0.160319    0.460262\n",
       "  0.545883    -0.199483    0.514127      -4.24455    -4.20667    -4.14337\n",
       "  2.15259      0.0522619   0.0675038  …  -0.0296104  -0.0392038  -0.0438145\n",
       " -0.433061    -0.985746   -0.513366       0.368157    0.616549    0.803371\n",
       "  0.00152145   0.997358    1.42222       -0.440633   -0.85439    -0.964204\n",
       "  2.71532     -0.906451    0.384201      -1.44782    -1.46486    -1.43477\n",
       "  1.82701      1.27142    -1.15566        1.04111     0.849804    0.684699\n",
       " -2.36265     -1.24984    -1.98358    …   1.06401     1.02021     1.00714\n",
       " -0.00173678  -1.83543     0.815664      -2.21205    -2.00926    -1.76395\n",
       "  1.65709     -1.17497     0.843728      -1.07015    -1.11008    -0.99317"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = transformer[1]\n",
    "emb = embedding(tok=E, segment=seg_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Above we have embedding corresponding to input indices**\n",
    "\n",
    "*lets pass the embedding through AlbertTranformer to get contextualised_embedding*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### voilà we got Contextualised embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768×32×3 Array{Float32,3}:\n",
       "[:, :, 1] =\n",
       "  0.304567    0.405941    0.326642   …   0.527699    0.53062     0.535387\n",
       "  0.765856    1.05468     0.581975       0.902422    0.923356    0.910047\n",
       "  0.760675    0.440686    1.81601        0.665885    0.615429    0.596886\n",
       " -0.0437517  -0.464514   -0.133041      -0.0119426   0.0575484   0.0867782\n",
       "  0.438816   -0.0251722   0.686429       0.80559     0.805196    0.780889\n",
       "  0.0296612  -0.685908    0.21051    …  -0.175856   -0.189034   -0.196193\n",
       "  0.646657    1.67699     0.259575       0.684573    0.667937    0.646987\n",
       "  0.267509    1.71205     0.686641       0.999966    1.0456      1.06525\n",
       " -0.0601687  -0.354132   -0.886672      -0.617012   -0.610717   -0.594557\n",
       " -0.364813   -0.159958   -0.996861      -0.187835   -0.205887   -0.215373\n",
       "  0.988467    1.11282     1.00583    …   0.78229     0.908326    0.959077\n",
       "  0.0731863  -0.432719    0.150405       0.585053    0.49542     0.441913\n",
       "  0.15807    -0.490142   -0.405144       0.355488    0.33691     0.322694\n",
       "  ⋮                                  ⋱               ⋮          \n",
       " -0.61483    -0.321608   -0.396901      -0.836611   -0.853394   -0.865005\n",
       " -0.0746915   0.738938    0.832527       0.342028    0.363095    0.365716\n",
       " -0.655902   -1.1607     -1.03446       -0.329239   -0.297827   -0.290003\n",
       " -0.174335   -0.143617   -0.398854      -0.528827   -0.561043   -0.585855\n",
       " -1.07943    -1.33676    -1.64727    …  -0.709648   -0.729482   -0.749425\n",
       " -0.26735     0.51161    -0.178264      -0.526058   -0.522488   -0.503441\n",
       " -0.0936105   0.775895   -0.531953       0.591429    0.477617    0.428794\n",
       " -0.41692    -0.290545   -0.37668       -0.810094   -0.902085   -0.958635\n",
       " -1.44747    -0.655274    0.0473042     -1.65663    -1.70124    -1.72883\n",
       " -0.451427   -1.83576    -0.306356   …  -1.21235    -1.20466    -1.18371\n",
       "  0.624991    0.149222    0.725459       0.518492    0.579501    0.592086\n",
       " -0.440716    0.428943    0.599384      -0.336462   -0.38008    -0.395833\n",
       "\n",
       "[:, :, 2] =\n",
       " -0.09183    -0.131673   -0.0800453  …   0.173828    0.658507    0.701122\n",
       " -0.212849    0.425527    0.468926      -0.0383679  -0.254274   -0.0674611\n",
       " -0.341473   -0.734532    0.511896      -0.411946    0.0223538   0.465937\n",
       " -0.567055   -0.482274   -0.578492      -1.64395    -0.918566   -1.23404\n",
       " -0.0708104  -0.59162     0.669678       0.321576    1.18997     0.353643\n",
       " -0.0727395  -0.0380078   0.356677   …   0.726067    0.455232   -0.426506\n",
       "  1.07172     1.92992     1.11336        0.585416   -1.38213     0.764737\n",
       " -0.0939165   1.50271     0.300908      -0.608412   -0.663113    0.571664\n",
       "  0.934051   -0.303075    0.715795       0.201148    0.378836    0.556202\n",
       "  1.15065     1.58561     1.01433        0.123385    0.436604    0.416501\n",
       "  0.385488    0.836604   -0.429251   …   1.30753    -0.948893    0.815243\n",
       " -0.262729   -1.29692    -0.795957       0.492651    0.310448   -0.779165\n",
       " -0.182217   -0.979907   -0.463309      -0.604354    0.542724   -1.3294\n",
       "  ⋮                                  ⋱               ⋮          \n",
       "  0.313161    0.297878    0.238105      -1.24137    -0.959478   -0.116641\n",
       " -0.212055    0.738047    0.413007       0.364323   -0.160887   -0.25063\n",
       "  0.347592    0.115097   -0.10026       -0.299359   -0.711259   -0.252505\n",
       "  0.0245221  -0.190721    0.824076      -1.36732    -1.30632     0.85736\n",
       "  0.251426   -0.0943026   0.19253    …   0.456811    0.0460289   0.700604\n",
       " -0.792393   -0.963726   -0.707541      -1.5676     -1.30831    -1.06157\n",
       " -0.78274    -0.218586   -0.905263      -0.0916421  -0.39057    -0.315191\n",
       " -0.255523   -0.509181    0.0483524      0.51547    -0.299128   -0.54637\n",
       " -2.5418     -2.05595    -1.71871        0.58241    -0.680839   -3.12875\n",
       "  0.0385001  -0.665508   -0.385825   …  -0.494911   -0.0957252  -0.558129\n",
       "  0.511562    0.140494    0.752053      -0.387508   -0.0610264   0.639027\n",
       " -0.988561   -0.974358   -0.338162      -0.871169   -1.64719    -1.42307\n",
       "\n",
       "[:, :, 3] =\n",
       " -0.464287    0.444703    0.332893   …   0.674975     0.69247     0.707714\n",
       "  1.01715     0.517691    0.078989       0.692509     0.75075     0.755857\n",
       " -0.149221   -0.315486    0.943656       0.896174     0.823444    0.786195\n",
       " -0.775787   -0.575133   -0.768595       0.114394     0.155716    0.157084\n",
       " -0.763795    0.167911    0.860362       0.478681     0.448441    0.438333\n",
       "  0.696022   -1.07504     0.214933   …  -0.923638    -0.924721   -0.921711\n",
       "  0.553646    1.40984     1.25859        0.903849     0.887553    0.890889\n",
       " -0.788586    0.565872   -0.174171       0.592233     0.678486    0.703608\n",
       "  0.917794   -0.290047    0.270431      -0.577756    -0.557005   -0.531713\n",
       "  0.0703508   0.718436    0.524612      -0.106808    -0.161638   -0.198985\n",
       "  1.83724     1.25059     0.441462   …   0.717154     0.842766    0.873928\n",
       " -1.00134    -1.23632    -0.182          0.975941     0.873459    0.825883\n",
       "  0.469249   -0.668738   -0.176839      -0.00468119  -0.0277034  -0.0290249\n",
       "  ⋮                                  ⋱                ⋮          \n",
       " -1.23669    -0.380987   -1.42546       -1.55032     -1.53026    -1.51031\n",
       "  0.208317    0.0787398   0.419125       0.865003     0.916697    0.938971\n",
       " -0.626502   -0.560398   -0.64055       -0.569054    -0.532433   -0.525647\n",
       "  0.374139   -0.852387    0.234086      -0.76758     -0.827931   -0.849001\n",
       " -0.499433   -0.115525   -1.0104     …  -0.357703    -0.374017   -0.408415\n",
       " -0.930513   -0.615282   -1.221         -0.550265    -0.580068   -0.5931\n",
       "  0.812305   -0.594478   -1.84857        0.303941     0.145331    0.0992277\n",
       "  0.103422   -0.172053    0.0998552     -0.925473    -1.03992    -1.10758\n",
       " -1.46715    -1.21069    -0.278395      -1.86522     -1.91097    -1.96085\n",
       " -0.464279   -1.01199     0.0595983  …  -1.23196     -1.19695    -1.17389\n",
       " -0.463837   -0.709276    1.26165       -0.606749    -0.481565   -0.447495\n",
       " -0.407867    0.342045    0.423647      -0.871686    -0.89004    -0.880378"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextualised_embedding = transformer[2](emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense(768, 768, tanh)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls = transformer[3][1] #pooler layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transform = Chain(Dense(768, 128, gelu), LayerNorm(128)), output_bias = Float32[-5.345022, 2.1769698, -7.144285, -9.102521, -8.083536, 0.56541324, 1.2000155, 1.4699979, 1.5557922, 1.9452884  …  -0.6403663, -0.9401073, -1.0888876, -0.9298268, -0.64744073, -0.47156653, -0.81416136, -0.87479985, -0.8785063, -0.5505797])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls = transformer[3][2] #for mlm  tasked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Dense(768, 2), logsoftmax)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls = transformer[3][3] #for Sentence order prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768×3 Array{Float32,2}:\n",
       " -0.164622   -0.244687   -0.337835\n",
       " -0.109313   -0.238238    0.0731121\n",
       " -0.922526   -0.99792    -0.46698\n",
       " -0.403019   -0.863416    0.702999\n",
       "  0.439029    0.97862    -0.107664\n",
       " -0.852264   -0.900703   -0.614105\n",
       "  0.364179   -0.373978    0.0256299\n",
       " -0.508897   -0.47372     0.164483\n",
       "  0.536143    0.0399534   0.775943\n",
       "  0.970573    0.768918    0.205684\n",
       "  0.999793    0.998327    0.999979\n",
       "  0.791798   -0.33849     0.438395\n",
       " -0.503646    0.227776    0.245557\n",
       "  ⋮                      \n",
       "  0.757302    0.637661    0.859677\n",
       " -0.819503   -0.998739   -0.814534\n",
       " -0.505278   -0.654982   -0.670874\n",
       " -0.994383    0.0932321  -0.308428\n",
       " -0.6843     -0.984353   -0.602701\n",
       "  0.751023    0.684121    0.818537\n",
       " -0.997563   -0.990052   -0.98816\n",
       " -0.405217   -0.750618   -0.819593\n",
       "  0.53443     0.589601    0.67484\n",
       "  0.973679    0.97328     0.951646\n",
       " -0.880809   -0.918532    0.0197378\n",
       " -0.0609629  -0.814388   -0.487664"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Transformers.Basic:@toNd\n",
    "transformer[3].pooler(contextualised_embedding[:,1,:]) #mostly pooler layer is applied on [cls] token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see our tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Array{String,1},1}:\n",
       " [\"_\", \"G\", \"od\", \"_is\", \"_\", \"G\", \"re\", \"at\", \"!\", \"_\", \"I\", \"_won\", \"_a\", \"_lottery\", \".\"]\n",
       " [\"_\", \"I\", \"f\", \"_all\", \"_their\", \"_conversations\", \"_in\", \"_the\", \"_three\", \"_months\"  …  \"_was\", \"_doubtful\", \"_that\", \"_they\", \"_would\", \"_make\", \"_a\", \"_respectable\", \"_paragraph\", \".\"]\n",
       " [\"_\", \"S\", \"he\", \"_had\", \"_the\", \"_job\", \"_she\", \"_had\", \"_planned\", \"_for\", \"_the\", \"_last\", \"_three\", \"_years\", \".\"]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 =  tokenizer(spm,sample[1])\n",
    "s2 =  tokenizer(spm,sample[2])\n",
    "s3 =  tokenizer(spm,sample[3])\n",
    "s =[s1,s2,s3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×32×3 Array{Float32,3}:\n",
       "[:, :, 1] =\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 2] =\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  …  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
       "\n",
       "[:, :, 3] =\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Transformers\n",
    "masks = Transformers.Basic.getmask(s) # we can directly use getmask function of Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768×32×3 Array{Float32,3}:\n",
       "[:, :, 1] =\n",
       " -0.428328   -0.0777661  -0.221679    …   0.0   0.0   0.0   0.0   0.0   0.0\n",
       " -0.569262    0.571153   -0.281473        0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.0193166   0.0451773   2.74292         0.0   0.0   0.0   0.0   0.0   0.0\n",
       " -0.978615   -1.16123    -0.574968       -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       "  0.201378   -0.735267    0.0164461       0.0   0.0   0.0   0.0   0.0   0.0\n",
       " -0.322908   -0.379362   -0.813322    …  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       "  0.660336    2.02064     0.649449        0.0   0.0   0.0   0.0   0.0   0.0\n",
       " -0.91708     0.392511   -0.667269       -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       " -0.254425   -0.426472   -1.18446        -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       "  0.617348    1.62654     0.141215        0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  1.55256     2.01011     1.37082     …   0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.0662555  -0.670731    0.121412       -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       "  0.127238   -0.539187   -0.00406911     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       "  ⋮                                   ⋱                           ⋮    \n",
       " -0.718619   -0.671933   -0.338205       -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       " -0.170883    0.413739    0.874195        0.0   0.0   0.0   0.0   0.0   0.0\n",
       " -0.0445368  -0.878869   -0.870303       -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       " -0.524704   -1.17009    -0.694465       -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       " -0.057944   -0.264543   -1.03365     …  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       " -0.636183   -0.266041   -0.757956       -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       " -1.06816     0.959581   -1.48805        -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       " -0.491942   -0.131904   -1.00575        -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       " -1.96159    -0.728627    0.151455       -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       " -0.319676   -1.01996     0.214987    …  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       "  1.09966    -0.315775    0.314053       -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       " -1.05864    -0.821646   -0.826332       -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       "\n",
       "[:, :, 2] =\n",
       " -0.09183    -0.131673   -0.0800453  …   0.173828    0.658507    0.701122\n",
       " -0.212849    0.425527    0.468926      -0.0383679  -0.254274   -0.0674611\n",
       " -0.341473   -0.734532    0.511896      -0.411946    0.0223538   0.465937\n",
       " -0.567055   -0.482274   -0.578492      -1.64395    -0.918566   -1.23404\n",
       " -0.0708104  -0.59162     0.669678       0.321576    1.18997     0.353643\n",
       " -0.0727395  -0.0380078   0.356677   …   0.726067    0.455232   -0.426506\n",
       "  1.07172     1.92992     1.11336        0.585416   -1.38213     0.764737\n",
       " -0.0939165   1.50271     0.300908      -0.608412   -0.663113    0.571664\n",
       "  0.934051   -0.303075    0.715795       0.201148    0.378836    0.556202\n",
       "  1.15065     1.58561     1.01433        0.123385    0.436604    0.416501\n",
       "  0.385488    0.836604   -0.429251   …   1.30753    -0.948893    0.815243\n",
       " -0.262729   -1.29692    -0.795957       0.492651    0.310448   -0.779165\n",
       " -0.182217   -0.979907   -0.463309      -0.604354    0.542724   -1.3294\n",
       "  ⋮                                  ⋱               ⋮          \n",
       "  0.313161    0.297878    0.238105      -1.24137    -0.959478   -0.116641\n",
       " -0.212055    0.738047    0.413007       0.364323   -0.160887   -0.25063\n",
       "  0.347592    0.115097   -0.10026       -0.299359   -0.711259   -0.252505\n",
       "  0.0245221  -0.190721    0.824076      -1.36732    -1.30632     0.85736\n",
       "  0.251426   -0.0943026   0.19253    …   0.456811    0.0460289   0.700604\n",
       " -0.792393   -0.963726   -0.707541      -1.5676     -1.30831    -1.06157\n",
       " -0.78274    -0.218586   -0.905263      -0.0916421  -0.39057    -0.315191\n",
       " -0.255523   -0.509181    0.0483524      0.51547    -0.299128   -0.54637\n",
       " -2.5418     -2.05595    -1.71871        0.58241    -0.680839   -3.12875\n",
       "  0.0385001  -0.665508   -0.385825   …  -0.494911   -0.0957252  -0.558129\n",
       "  0.511562    0.140494    0.752053      -0.387508   -0.0610264   0.639027\n",
       " -0.988561   -0.974358   -0.338162      -0.871169   -1.64719    -1.42307\n",
       "\n",
       "[:, :, 3] =\n",
       " -0.23863    -0.0402696  -0.0582475   …  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       "  0.248931    0.745041   -0.224358       -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       " -0.168032   -0.325815    0.67951        -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       " -0.219288   -1.1247     -0.909867       -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       " -0.514198   -0.555745    0.331553       -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       "  0.0987283  -0.689964    1.09279     …  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       " -0.0189531   0.767154    1.22242        -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       " -0.713311    0.590924   -0.633828       -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       "  0.73015    -0.564487   -0.392198        0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.392737    1.55139     0.370175        0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.864051    1.36961     0.488098    …   0.0   0.0   0.0   0.0   0.0   0.0\n",
       " -1.07923    -1.52293    -0.207542        0.0   0.0   0.0   0.0   0.0   0.0\n",
       " -0.171224   -0.519082   -0.619283        0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  ⋮                                   ⋱                           ⋮    \n",
       " -0.159864    0.143103   -0.705577       -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       " -0.129784    0.464316   -0.00841472      0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.108938   -0.525286    0.222788        0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.056077   -1.12363    -0.292434       -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       " -0.0774956  -0.117729    0.0798739   …   0.0   0.0   0.0   0.0   0.0   0.0\n",
       " -0.662674   -0.546186   -1.49554        -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       " -0.77475    -0.956721   -2.56259        -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       " -0.055788   -0.634893   -0.197342       -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       " -1.58012    -1.18332    -0.045383       -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       " -0.0288196  -0.923488   -0.250819    …   0.0   0.0   0.0   0.0   0.0   0.0\n",
       "  0.492049    0.0397632   0.554722       -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n",
       " -1.03942    -0.858426   -0.0576648      -0.0  -0.0  -0.0  -0.0  -0.0  -0.0"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextualised_embedding = transformer[2](emb, masks) #contextualised_embedding with masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Wait\n",
    "It is just like an other Flux layers/ Structure means we can easily train it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([Float32[1.4228282, 1.3738428, 1.4611361, 1.4545419, 1.362615, 1.4506888, 1.378717, 1.5180601, 1.4845772, 1.3975797  …  1.4648936, 1.0347399, 1.3678652, 1.4229372, 1.4521085, 1.5046557, 1.5158035, 1.4426574, 1.4960195, 1.3573524], Float32[-0.053622514, -0.16088338, 0.22308606, 0.048916165, -0.0039820396, 0.099344134, -0.0748515, 0.039203927, 0.12206718, 0.05795675  …  -0.27271414, 0.8688461, -0.04732636, -0.18536331, -0.16637102, 0.1740389, -0.294258, -0.13245566, -0.00337506, -0.345267], Float32[-0.051017728 0.086519726 … -0.11926416 0.07093989; -0.056381047 0.022605542 … -0.11318762 -0.11180934; … ; -0.1064435 -0.05314829 … 0.087794125 0.1817395; -0.063876376 -0.0543424 … 0.22770554 -0.03343155], Float32[0.0027943964 -0.0062411674; -0.01266096 0.017194653; … ; 0.0003534697 0.0020992558; 0.044360843 -0.0007803185], Float32[-0.027650086 0.004228708 … -0.022506572 -0.05371121; -0.058056954 -0.009886336 … -0.007817211 0.05937399; … ; -0.061029218 -0.050291896 … 0.047393326 0.080278724; 0.00067498547 -0.031160265 … -0.067646295 -0.17033984], Float32[-0.02763474 0.06171614 … -0.06946765 0.052770674; -0.0069716414 0.07572139 … 0.023323966 -0.0075935223; … ; -0.058513153 0.040454373 … 0.010056733 -0.006323604; 0.041100588 0.03806929 … 0.019924467 -0.015943302], Float32[-0.14952344, 0.16533042, -0.2101735, -0.04594988, 0.23513977, -0.068325184, -0.012892563, 0.087248124, 0.040170096, 0.25634593  …  -0.0183876, 0.08122287, -0.07749175, -0.08307301, 0.19914186, -0.011005787, 0.259376, 0.04787546, -0.007221762, 0.17180045], Float32[-0.015541519 -0.040830243 … -0.0077291154 0.040761676; -0.0063388054 0.047942817 … -0.015170522 0.017620556; … ; 0.09501812 -0.078597255 … -0.044319842 0.020874618; 0.011273312 0.044222906 … 0.024886178 0.049045883], Float32[-0.012893528, -0.042182963, 0.08626874, -1.2130626, -0.37122118, 0.38116136, 0.09824966, -0.1516113, 0.19198471, -0.20457509  …  0.29980165, -0.12774986, -0.11536464, 0.38857827, 0.096464284, -0.6301059, -0.02738171, -0.17149079, 0.48313934, 0.07648202], Float32[-0.062012766 -0.0038790156 … -0.013586831 -0.05443526; -0.03617773 -0.06059524 … 0.013859719 0.036572225; … ; 0.05103405 0.023533288 … -0.025796792 -0.044482462; -0.045234073 0.026275402 … 0.02872855 0.02600806], Float32[-0.55470765, 0.2635302, -0.22981001, -0.35598248, -0.6882087, 0.1314481, 0.4750412, -0.20286554, 0.36496747, 0.08039843  …  0.07559351, 0.13412842, -0.3392635, -0.1114853, 0.24200334, 0.024432966, 0.2565072, -0.30036825, 0.02098731, 0.13001953], Float32[0.0073378617 0.014102026 … 0.05130136 0.05354844; 0.043158643 0.010668716 … 0.036385715 0.05625187; … ; -0.04241124 0.024859397 … 0.00043820313 0.054751452; -0.15118377 -0.041334566 … -0.053102855 -0.008044204], Float32[-0.06423594, -0.06581749, -0.053180527, -0.016467236, -0.05628329, 0.048142936, -0.048853435, 0.010226046, 0.0068836655, 0.0037812085  …  -0.022471834, -0.06257648, -0.057152558, -0.08077025, -0.021834247, 0.036149558, -0.08861526, -0.08393449, -0.05538553, -0.012941372], Float32[0.057780296 -0.06299273 … -0.0031617403 0.045156226; 0.015923511 -0.065165 … 0.062196046 0.07028379; … ; 0.0072812764 0.03835821 … -0.036383778 0.094710544; -0.0105332015 -0.0017958063 … -0.08351556 0.08560134], Float32[-0.096343465, 0.047691695, 0.040329278, 0.052683406, -0.06760634, 0.035319563, 0.06344245, 0.0236313, -0.018780394, 0.088613056  …  0.009888015, -0.08970711, 0.012949983, 0.013905061, -0.0061980337, 0.09738326, -0.04150353, 0.14649425, 0.045280404, -0.033034664], Float32[0.5328006, 0.592023, 0.75605094, 0.7033415, 0.5889806, 0.71082014, 0.6810446, 0.64183426, 0.60005605, 0.53903514  …  0.75633764, 0.71564883, 0.5630059, 0.58312756, 0.7922718, 0.5806809, 0.80658364, 0.6468443, 0.6646046, 0.6359836], Float32[-0.0051945574, -0.02807719, 0.04571535, -0.019613288, 0.027921112, -0.090160616, 0.010318609, 0.0076556173, 0.05001609, 0.044297367  …  0.05905909, -0.03741357, -0.029315917, -0.027294774, 0.11878953, -0.016295554, 0.019110713, 0.027869994, -0.04762518, -0.0034988567], Float32[0.06863134 -0.0643824 … -0.03783059 0.025742829; -0.0389544 -0.028703514 … 0.038237378 -0.034218177; … ; 0.022287473 -0.11799408 … 0.045260143 0.020802613; -0.021115236 -0.0007448066 … 0.0503633 0.024318503], Float32[-0.2780281, -0.13797009, 0.12554154, 0.0712129, 0.12978739, -0.2967079, -0.07025385, -0.30991563, -0.17584197, 0.09694058  …  0.17783846, -0.11816767, -0.09382434, -0.102314524, -0.037760105, -0.005960665, -0.24526832, 0.24916978, -0.18387622, -0.621726], Float32[-0.016856294 0.026972152 … 0.016366126 0.05188403; 0.0555499 -0.019615026 … 0.0667124 0.007247157; … ; 0.0032954449 0.007028783 … 0.037165944 -0.03341696; 0.059301525 0.021443704 … 0.022085855 0.04806439], Float32[0.13354924, -0.065206915, 0.1356805, 0.06264471, -0.08415001, -0.012828431, 0.1537846, 0.050802343, -0.084149, 0.18080558  …  0.0505975, -0.16209385, -0.06986705, -0.12173515, -0.005052082, 0.21664703, 0.11252434, 0.016340183, -0.17727618, 0.112120256], Float32[0.81200194, 0.8466851, 0.8926652, 0.8863681, 0.77376986, 0.8258225, 0.9532571, 0.9124772, 0.84209985, 0.78845024  …  0.84126425, 0.8627402, 0.80095136, 0.8296966, 0.89858323, 0.86364573, 0.86447245, 0.8235725, 0.88896865, 0.8741806], Float32[-0.023961732, -0.046634357, -0.0143565945, -0.006937244, -0.038529433, 0.05300232, -0.05633876, -0.07111825, -0.06697029, -0.017047023  …  -0.07388534, 0.007005827, 0.02576593, -0.028130839, -0.06284993, -0.0070077395, -0.028038977, -0.02892837, 0.062209304, -0.06736273], Float32[0.025437102 0.0064323233 … -0.0037462313 0.02546577; -0.0028480298 -0.024373451 … -0.03299369 0.0071955877; … ; -0.06819669 -0.02772355 … 0.012815827 -0.003786622; 0.00038827205 -0.009547479 … -0.03671143 0.022545092], Float32[-0.029056959, -0.015764097, -0.3193157, -0.052037634, -0.0068009645, -0.14643799, 0.040107176, -0.005595347, 0.016801842, -0.19551551  …  -0.0131766, -0.025098212, -0.24818766, 0.01688845, -0.06429409, -0.04713422, 0.013679432, 0.09079702, -0.109263085, -0.00039715314], Float32[-0.031208588 0.0091013815 … -0.00684734 -0.026423937; 0.028685097 -0.01848697 … 0.023392398 -0.04492154; … ; 0.0026264421 -0.037468694 … 0.051623445 -0.08754498; 0.004305736 0.013284024 … -0.010517346 -0.03307538], Float32[0.5673871, 0.5173458, 0.10752545, 0.3155134, 0.31712934, 0.47738948, 0.25802857, 0.5325181, 0.4967766, 0.21005148  …  0.48921984, 0.31953004, 0.44065377, 0.3762529, 0.35104525, 0.5068734, 0.46040258, 0.1726858, 0.18739718, 0.5938985], Float32[4.8125944, 4.4954085, 4.502573, 4.2537217, 3.9418182, 4.7133255, 3.674857, 4.635417, 4.365962, 4.603444  …  4.4542103, 4.589312, 4.430569, 3.6093228, 4.211171, 4.285142, 4.370338, 4.2291827, 4.6284213, 4.399151], Float32[1.4715962, 0.9182028, -2.7345228, -2.2219348, -2.1531718, 1.349703, -2.5489144, 1.511412, 1.9419745, -2.2967346  …  1.2422282, 1.2655686, 1.3246479, -2.4911828, -1.340733, 1.3877705, 1.5082483, -2.0916762, -2.2163546, 1.49516], Float32[-5.345022, 2.1769698, -7.144285, -9.102521, -8.083536, 0.56541324, 1.2000155, 1.4699979, 1.5557922, 1.9452884  …  -0.6403663, -0.9401073, -1.0888876, -0.9298268, -0.64744073, -0.47156653, -0.81416136, -0.87479985, -0.8785063, -0.5505797], Float32[0.002305334 -0.016102044 … -0.0335534 0.0028744815; 0.0025165232 -0.015944447 … 0.023191692 0.002694243], Float32[-0.020070702, 0.039414737]])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux: params\n",
    "params(transformer) #parameters can be updated easily "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take some datasets present in Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Array{String,1},1}:\n",
       " [\"When did the third Digimon series begin?\", \"Which missile batteries often have individual launchers several kilometres from one another?\", \"What two things does Popper argue Tarski's theory involves in an evaluation of truth?\", \"What is the name of the village 9 miles north of Calafat where the Ottoman forces attacked the Russians?\", \"What famous palace is located in London?\", \"When is the term 'German dialects' used in regard to the German language?\", \"What was the name of the island the English traded to the Dutch in return for New Amsterdam?\", \"How were the Portuguese expelled from Myanmar?\", \"What does the word 'customer' properly apply to?\", \"What did Arsenal consider the yellow and blue colors to be after losing a FA Cup final wearing red and white?\"  …  \"Which of Calatrava's creations contains an IMAX theater?\", \"What is Seattle's average December temperature?\", \"Bell learned to accurately read lips even without knowing what?\", \"What is Oklahoma's largest school district?\", \"What is a name for the reduced complement of genetic material necessary for an organism to live?\", \"In what year was the Mananga Management Centre founded?\", \"What rank provided its holder territorial rule?\", \"What happened to Hornswoggle?\", \"What percentage of Mexico City's population was indigenous in 1921?\", \"In what year did a French magazine describe the use of asphalt?\"]\n",
       " [\"Unlike the two seasons before it and most of the seasons that followed, Digimon Tamers takes a darker and more realistic approach to its story featuring Digimon who do not reincarnate after their deaths and more complex character development in the original Japanese.\", \"When MANPADS is operated by specialists, batteries may have several dozen teams deploying separately in small sections; self-propelled air defence guns may deploy in pairs.\", \"He bases this interpretation on the fact that examples such as the one described above refer to two things: assertions and the facts to which they refer.\", \"On 31 December 1853, the Ottoman forces at Calafat moved against the Russian force at Chetatea or Cetate, a small village nine miles north of Calafat, and engaged them on 6 January 1854.\", \"London contains four World Heritage Sites: the Tower of London; Kew Gardens; the site comprising the Palace of Westminster, Westminster Abbey, and St Margaret's Church; and the historic settlement of Greenwich (in which the Royal Observatory, Greenwich marks the Prime Meridian, 0° longitude, and GMT).\", \"When talking about the German language, the term German dialects is only used for the traditional regional varieties.\", \"At the end of the Second Anglo-Dutch War, the English gained New Amsterdam (New York) in North America in exchange for Dutch control of Run, an Indonesian island.\", \"From the 1720s onward, the kingdom was beset with repeated Meithei raids into Upper Myanmar and a nagging rebellion in Lan Na.\", \"The bill also required rotation of principal maintenance inspectors and stipulated that the word \\\"customer\\\" properly applies to the flying public, not those entities regulated by the FAA.\", \"Arsenal then competed in three consecutive FA Cup finals between 1978 and 1980 wearing their \\\"lucky\\\" yellow and blue strip, which remained the club's away strip until the release of a green and navy away kit in 1982–83.\"  …  \"i les Ciències), which contains an opera house/performing arts centre, a science museum, an IMAX cinema/planetarium, an oceanographic park and other structures such as a long covered walkway and restaurants.\", \"Winters are cool and wet with December, the coolest month, averaging 40.6 °F (4.8 °C), with 28 annual days with lows that reach the freezing mark, and 2.0 days where the temperature stays at or below freezing all day; the temperature rarely lowers to 20 °F (−7 °C).\", \"In this treatise, his father explains his methods of how to instruct deaf-mutes (as they were then known) to articulate words and read other people's lip movements to decipher meaning.\", \"Oklahoma City is home to the state's largest school district, Oklahoma City Public Schools.\", \"There is experimental work being done on minimal genomes for single cell organisms as well as minimal genomes for multi-cellular organisms (see Developmental biology).\", \"The Mananga management centre was established as Mananga Agricultural Management Centre in 1972 as an International Management Development Centre catering for middle and senior managers, it is located at Ezulwini.\", \"Each successive rank gave its holder greater pensions and legal privileges.\", \"Dave Finlay was often aided in his matches by a midget known mainly as Hornswoggle while in WWE, who hid under the ring and gave a shillelagh to Finlay to use on his opponent.\", \"In 1921, Mexico City had less than one million inhabitants.\", \"One hundred years after the fall of Constantinople in 1453, Pierre Belon described in his work Observations in 1553 that pissasphalto, a mixture of pitch and bitumen, was used in Dubrovnik for tarring of ships from where it was exported to a market place in Venice where it could be bought by anyone.\"]\n",
       " [\"not_entailment\", \"not_entailment\", \"entailment\", \"entailment\", \"not_entailment\", \"entailment\", \"entailment\", \"not_entailment\", \"entailment\", \"entailment\"  …  \"not_entailment\", \"entailment\", \"not_entailment\", \"entailment\", \"entailment\", \"entailment\", \"not_entailment\", \"not_entailment\", \"not_entailment\", \"not_entailment\"]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Transformers.Datasets\n",
    "using Transformers.Datasets.GLUE\n",
    "\n",
    "task = GLUE.QNLI()\n",
    "datas = dataset(Train, task)\n",
    "training_batch=get_batch(datas, 100) #here 100 correspond to no. of sentence for train we output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preprocess (generic function with 1 method)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux: onehotbatch\n",
    "makesentence(s1, s2) = [\"[CLS]\"; s1; \"[SEP]\"; s2; \"[SEP]\"]\n",
    "function preprocess(training_batch)\n",
    "ids =[]\n",
    "sent = []\n",
    "for i in 11:35\n",
    "    sent1 = tokenizer(spm,training_batch[1][i])\n",
    "    sent2 = tokenizer(spm,training_batch[2][i])\n",
    "    id = makesentence(sent1,sent2)\n",
    "    push!(sent, id)\n",
    "    push!(ids,ids_from_tokens(spm,id))\n",
    "end\n",
    "E = Flux.batchseq(ids,1)\n",
    "E = Flux.stack(E,1)\n",
    "segment = fill!(similar(E), 1)\n",
    "    for (i, sent) ∈ enumerate(sent)\n",
    "      j = findfirst(isequal(\"[SEP]\"), sent)\n",
    "      if j !== nothing\n",
    "        @view(segment[j+1:end, i]) .= 2\n",
    "      end\n",
    "end\n",
    "segment\n",
    "data = (tok = E,segment = segment)\n",
    "labels = get_labels(task)\n",
    "label = onehotbatch(training_batch[3][11:35], labels)\n",
    "return(data,label)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tok = [3 3 … 3 3; 14 14 … 14 14; … ; 1 1 … 1 1; 1 1 … 1 1], segment = [1 1 … 1 1; 1 1 … 1 1; … ; 2 2 … 2 2; 2 2 … 2 2]), Bool[0 0 … 0 0; 1 1 … 1 1])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data,label = preprocess(training_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134×25 Array{Int64,2}:\n",
       "    3     3     3     3      3      3  …     3     3     3      3      3\n",
       "   14    14    14    14     14     14       14    14    14     14     14\n",
       "    2     2     2     2      2      2        2     2     2      2      2\n",
       " 1808  1808  6776   253   1823   3582     5609   104  6776   6776   6776\n",
       " 5069    24   108  3871    414    213       47    99    26    145   1249\n",
       "   20   889   128  1207    700  20270  …  5092   429    15     15     24\n",
       "   14    29   369    17     27    415     1538    24   982  10481   2745\n",
       "   23    15   566  1690  13926     93     1705    15    31   2167     21\n",
       "    2    14  2663  4841     16     45       61   325  3807     29     58\n",
       "  140     2  1995    51     99   6757        4   614    21     22  10955\n",
       " 4186    59    61  1031    963     61  …    14    36   925    287     16\n",
       "   14  9745     4  6271     93      4        2    15   114    393   3994\n",
       "    2    14    14    21   4432     14      104  1475    41    141   6128\n",
       "    ⋮                               ⋮  ⋱     ⋮                     \n",
       "    1     1     1     1      1      1        1     1     1      1      1\n",
       "    1     1     1     1      1      1        1     1     1      1      1\n",
       "    1     1     1     1      1      1        1     1     1      1      1\n",
       "    1     1     1     1      1      1  …     1     1     1      1      1\n",
       "    1     1     1     1      1      1        1     1     1      1      1\n",
       "    1     1     1     1      1      1        1     1     1      1      1\n",
       "    1     1     1     1      1      1        1     1     1      1      1\n",
       "    1     1     1     1      1      1        1     1     1      1      1\n",
       "    1     1     1     1      1      1  …     1     1     1      1      1\n",
       "    1     1     1     1      1      1        1     1     1      1      1\n",
       "    1     1     1     1      1      1        1     1     1      1      1\n",
       "    1     1     1     1      1      1        1     1     1      1      1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = Flux.batchseq(ids,1)\n",
    "E = Flux.stack(E,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134×25 Array{Int64,2}:\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  1  1  1  1\n",
       " 1  1  2  1  1  2  1  2  1  1  1  1  1  1  1  1  1  1  1  1  2  1  1  1  1\n",
       " ⋮              ⋮              ⋮              ⋮              ⋮           \n",
       " 2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
       " 2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
       " 2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
       " 2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
       " 2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
       " 2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
       " 2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
       " 2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
       " 2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
       " 2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
       " 2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
       " 2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment = fill!(similar(E), 1)\n",
    "    for (i, sent) ∈ enumerate(sent)\n",
    "      j = findfirst(isequal(\"[SEP]\"), sent)\n",
    "      if j !== nothing\n",
    "        @view(segment[j+1:end, i]) .= 2\n",
    "      end\n",
    "end\n",
    "segment\n",
    "data = (tok = E,segment = segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×25 Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}:\n",
       " 0  0  0  1  0  0  0  1  0  0  0  1  0  1  0  0  1  0  0  1  1  0  1  0  0\n",
       " 1  1  1  0  1  1  1  0  1  1  1  0  1  0  1  1  0  1  1  0  0  1  0  1  1"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux: onehotbatch\n",
    "labels = get_labels(task)\n",
    "label = onehotbatch(training_batch[3][11:35], labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 2 methods)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux\n",
    "using Flux: gradient\n",
    "import Flux.Optimise: update!\n",
    "\n",
    "clf = Flux.Chain(\n",
    "    Flux.Dropout(0.1),\n",
    "    Flux.Dense(768, length(labels)), Flux.logsoftmax\n",
    ")\n",
    "\n",
    "ps = params(transformer)\n",
    "opt = ADAM(1e-4)\n",
    "#define the loss\n",
    "function loss(data, label, mask=nothing)\n",
    "    e = transformer[1](data)\n",
    "    t = transformer[2](e)\n",
    "    l = logcrossentropy( label,\n",
    "         clf(\n",
    "            transformer[3].pooler(\n",
    "                t[:,1,:]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return l\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompositeEmbedding(tok = Embed(128), segment = Embed(128), pe = PositionEmbedding(128, max_len=512), postprocessor = Positionwise(LayerNorm(128), Dropout(0.1)))"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Dropout(0.1), Dense(768, 2), logsoftmax)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = Flux.Chain(\n",
    "    Flux.Dropout(0.1),\n",
    "    Flux.Dense(768, length(labels)), Flux.logsoftmax\n",
    ")\n",
    "clf2 = Flux.Chain(\n",
    "    Flux.Dropout(0.1),\n",
    "    Flux.Dense(768, length(labels)), Flux.logsoftmax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l = 2.0030386f0\n",
      "l = 1.7268517f0\n",
      "l = 1.6604753f0\n",
      "l = 1.5049707f0\n",
      "l = 1.8035961f0\n",
      "l = 1.8366543f0\n",
      "l = 1.6482123f0\n",
      "l = 1.7435868f0\n",
      "l = 1.906287f0\n",
      "l = 1.6901565f0\n",
      "l = 1.3915592f0\n",
      "l = 1.3359941f0\n",
      "l = 1.3426436f0\n",
      "l = 1.3728523f0\n",
      "l = 1.6148944f0\n"
     ]
    },
    {
     "ename": "BoundsError",
     "evalue": "BoundsError: attempt to access 134×25 Array{Int64,2} at index [Base.Slice(Base.OneTo(134)), 16:26]",
     "output_type": "error",
     "traceback": [
      "BoundsError: attempt to access 134×25 Array{Int64,2} at index [Base.Slice(Base.OneTo(134)), 16:26]",
      "",
      "Stacktrace:",
      " [1] throw_boundserror(::Array{Int64,2}, ::Tuple{Base.Slice{Base.OneTo{Int64}},UnitRange{Int64}}) at ./abstractarray.jl:537",
      " [2] checkbounds at ./abstractarray.jl:502 [inlined]",
      " [3] _getindex at ./multidimensional.jl:726 [inlined]",
      " [4] getindex(::Array{Int64,2}, ::Function, ::UnitRange{Int64}) at ./abstractarray.jl:980",
      " [5] top-level scope at In[290]:2"
     ]
    }
   ],
   "source": [
    "for i ∈ 1:24 # 24/2 training step, just for illustration\n",
    "data_batch = (tok = data.tok[:,i:i+10],segment= data.segment[:,i:i+10])\n",
    "label_batch = label[:,i:i+10]\n",
    "l = loss(data_batch, label_batch)\n",
    "i=i+2\n",
    "@show l\n",
    "  grad = gradient(()->l, ps)\n",
    "  update!(opt, ps, grad)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
