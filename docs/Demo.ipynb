{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work in Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installing** dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are keeping for now all the file of ALBERT in transformers.jl (it will Ultimately be added in TextAnalysis.jl with Transformers.jl as its dependency) we are using tokenize and Wordpiece(sentencepiece) from Albert which is based on Transformers.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: CUDAdrv.jl failed to initialize, GPU functionality unavailable (set JULIA_CUDA_SILENT or JULIA_CUDA_VERBOSE to silence or expand this message)\n",
      "└ @ CUDAdrv /home/iamtejas/.julia/packages/CUDAdrv/mCr0O/src/CUDAdrv.jl:69\n"
     ]
    }
   ],
   "source": [
    "using Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are taking 3 sentence as sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{String,1}:\n",
       " \"God is Great! I won a lottery.\"                                                                                                                                 \n",
       " \"If all their conversations in the three months he had been coming to the diner were put together, it was doubtful that they would make a respectable paragraph.\"\n",
       " \"She had the job she had planned for the last three years.\"                                                                                                      "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1 = \"God is Great! I won a lottery.\"\n",
    "sample2 = \"If all their conversations in the three months he had been coming to the diner were put together, it was doubtful that they would make a respectable paragraph.\"\n",
    "sample3 = \"She had the job she had planned for the last three years.\"\n",
    "sample = [sample1,sample2,sample3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALBERT uses SentencePiece for word segmentation and Sentence segementation.To get better performance we first tokenise the Sentences by space(using Tokenise function of Albert) and then use SentencePiece for Word Segmentation and Encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we first replace \" \"(space) with ' _' [space(U+2581)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decode split sentence. SentencePiece replace space with U+2581 and Store word in .vocab file with U+2581 as prefix .To tackle this we are using little trick as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "We will replace it with better implementation of SentencePiece "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"She ▁had ▁the ▁job ▁she ▁had ▁planned ▁for ▁the ▁last ▁three ▁years.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1] = replace(sample[1],\" \"=> \" ▁\")\n",
    "sample[2] = replace(sample[2],\" \"=> \" ▁\")\n",
    "sample[3] = replace(sample[3],\" \"=> \" ▁\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Can use WordPiece for Word Segmentation with ALBERT vocabulary as follow. \n",
    "working on the APIs for Wordpiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{String,1}:\n",
       " \"God ▁is ▁Great! ▁I ▁won ▁a ▁lottery.\"                                                                                                                                                      \n",
       " \"If ▁all ▁their ▁conversations ▁in ▁the ▁three ▁months ▁he ▁had ▁been ▁coming ▁to ▁the ▁diner ▁were ▁put ▁together, ▁it ▁was ▁doubtful ▁that ▁they ▁would ▁make ▁a ▁respectable ▁paragraph.\"\n",
       " \"She ▁had ▁the ▁job ▁she ▁had ▁planned ▁for ▁the ▁last ▁three ▁years.\"                                                                                                                      "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Transformers.BidirectionalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Array{String,1},1}:\n",
       " [\"god\", \"▁is\", \"▁great\", \"!\", \"▁i\", \"▁won\", \"▁a\", \"▁lottery\", \".\"]                                                                                                                                 \n",
       " [\"if\", \"▁all\", \"▁their\", \"▁conversations\", \"▁in\", \"▁the\", \"▁three\", \"▁months\", \"▁he\", \"▁had\"  …  \"▁was\", \"▁doubtful\", \"▁that\", \"▁they\", \"▁would\", \"▁make\", \"▁a\", \"▁respectable\", \"▁paragraph\", \".\"]\n",
       " [\"she\", \"▁had\", \"▁the\", \"▁job\", \"▁she\", \"▁had\", \"▁planned\", \"▁for\", \"▁the\", \"▁last\", \"▁three\", \"▁years\", \".\"]                                                                                      "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenise.(sample, Val(true))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{SubString{String},1}:\n",
       " \"<pad>\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = readlines(\"/home/iamtejas/Downloads/albert_xlarge_v1/albert_xlarge/30k-clean.vocab\")\n",
    "vocabnew = split.(vocab , \"\\t\")\n",
    "vo = [vocabnew[1][1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need Score of SentencePiece unigram model for Wordpiece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30001-element Array{String,1}:\n",
       " \"<pad>\"        \n",
       " \"<pad>\"        \n",
       " \"<unk>\"        \n",
       " \"[CLS]\"        \n",
       " \"[SEP]\"        \n",
       " \"[MASK]\"       \n",
       " \"(\"            \n",
       " \")\"            \n",
       " \"\\\"\"           \n",
       " \"-\"            \n",
       " \".\"            \n",
       " \"–\"            \n",
       " \"£\"            \n",
       " ⋮              \n",
       " \"▁predation\"   \n",
       " \"▁saviour\"     \n",
       " \"▁archivist\"   \n",
       " \"▁obverse\"     \n",
       " \"error\"        \n",
       " \"▁tyrion\"      \n",
       " \"▁addictive\"   \n",
       " \"▁veneto\"      \n",
       " \"▁colloquial\"  \n",
       " \"agog\"         \n",
       " \"▁deficiencies\"\n",
       " \"▁eloquent\"    "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in 1:30000\n",
    "    vocab1 = vocabnew[i][1]\n",
    "    push!(vo,vocab1)\n",
    "end\n",
    "vocab1 = convert(Array{String,1},vo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000-element Array{String,1}:\n",
       " \"<pad>\"        \n",
       " \"<unk>\"        \n",
       " \"[CLS]\"        \n",
       " \"[SEP]\"        \n",
       " \"[MASK]\"       \n",
       " \"(\"            \n",
       " \")\"            \n",
       " \"\\\"\"           \n",
       " \"-\"            \n",
       " \".\"            \n",
       " \"–\"            \n",
       " \"£\"            \n",
       " \"€\"            \n",
       " ⋮              \n",
       " \"▁predation\"   \n",
       " \"▁saviour\"     \n",
       " \"▁archivist\"   \n",
       " \"▁obverse\"     \n",
       " \"error\"        \n",
       " \"▁tyrion\"      \n",
       " \"▁addictive\"   \n",
       " \"▁veneto\"      \n",
       " \"▁colloquial\"  \n",
       " \"agog\"         \n",
       " \"▁deficiencies\"\n",
       " \"▁eloquent\"    "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab1 = vocab1[2:30001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordPiece(vocab_size=30000, unk=<unk>, max_char=200)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp = WordPiece(vocab1,\"<unk>\"; max_char = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Array{String,1},1}:\n",
       " [\"god\", \"▁is\", \"▁great\", \"!\", \"▁i\", \"▁won\", \"▁a\", \"▁lottery\", \".\"]                                                                                                                                 \n",
       " [\"if\", \"▁all\", \"▁their\", \"▁conversations\", \"▁in\", \"▁the\", \"▁three\", \"▁months\", \"▁he\", \"▁had\"  …  \"▁was\", \"▁doubtful\", \"▁that\", \"▁they\", \"▁would\", \"▁make\", \"▁a\", \"▁respectable\", \"▁paragraph\", \".\"]\n",
       " [\"she\", \"▁had\", \"▁the\", \"▁job\", \"▁she\", \"▁had\", \"▁planned\", \"▁for\", \"▁the\", \"▁last\", \"▁three\", \"▁years\", \".\"]                                                                                      "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokensnew = wp.(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{String,1}:\n",
       " \"God ▁is ▁Great! ▁I ▁won ▁a ▁lottery.\"                                                                                                                                                      \n",
       " \"If ▁all ▁their ▁conversations ▁in ▁the ▁three ▁months ▁he ▁had ▁been ▁coming ▁to ▁the ▁diner ▁were ▁put ▁together, ▁it ▁was ▁doubtful ▁that ▁they ▁would ▁make ▁a ▁respectable ▁paragraph.\"\n",
       " \"She ▁had ▁the ▁job ▁she ▁had ▁planned ▁for ▁the ▁last ▁three ▁years.\"                                                                                                                      "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocabulary(30000, unk=<unk>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabalbert = Transformers.Basic.Vocabulary(wp.vocab,wp.vocab[wp.unk_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we have indices form sample tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30×3 Array{Int64,2}:\n",
       "  5475    822  1080\n",
       "    26     66    42\n",
       "   375     67    15\n",
       "   188  13528  1206\n",
       "    32     20    40\n",
       "   231     15    42\n",
       "    22    133  2036\n",
       " 17519    819    27\n",
       "    10     25    15\n",
       "     2     42   237\n",
       "     2     75   133\n",
       "     2    881   123\n",
       "     2     21    10\n",
       "     ⋮             \n",
       "     2     16     2\n",
       "     2     33     2\n",
       "     2     24     2\n",
       "     2  22569     2\n",
       "     2     31     2\n",
       "     2     60     2\n",
       "     2     84     2\n",
       "     2    234     2\n",
       "     2     22     2\n",
       "     2  22740     2\n",
       "     2  20600     2\n",
       "     2     10     2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = vocabalbert(tokensnew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we only take one sentence as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30×3 Array{Int64,2}:\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " ⋮      \n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_indices = ones(Int, size(indices)...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to turn those indices into embeddings.These can be done by loading embedding in Embed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.5",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
