{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work in Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installing** dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are keeping for now all the file of ALBERT in transformers.jl (it will Ultimately be added in TextAnalysis.jl with Transformers.jl as its dependency) we are using tokenize and Wordpiece(sentencepiece) from Albert which is based on Transformers.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are taking 3 sentence as sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{String,1}:\n",
       " \"God is Great! I won a lottery.\"                                                                                                                                 \n",
       " \"If all their conversations in the three months he had been coming to the diner were put together, it was doubtful that they would make a respectable paragraph.\"\n",
       " \"She had the job she had planned for the last three years.\"                                                                                                      "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1 = \"God is Great! I won a lottery.\"\n",
    "sample2 = \"If all their conversations in the three months he had been coming to the diner were put together, it was doubtful that they would make a respectable paragraph.\"\n",
    "sample3 = \"She had the job she had planned for the last three years.\"\n",
    "sample = [sample1,sample2,sample3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALBERT uses SentencePiece for word segmentation and Sentence segementation.\n",
    "Here we are going to provide both Sentencepiece as well as Wordpiece (using Vocab file of ALBERT) for ALBERT with the former as defalut one.\n",
    "We first demonstrate WordPiece\n",
    "To use word piece we need to make following changes as below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we first replace \" \"(space) with ' _' [space(U+2581)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decode split sentence. SentencePiece replace space with U+2581 and Store word in .vocab file with U+2581 as prefix .To tackle this we are using little trick as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "syntax: extra token \"will\" after end of expression",
     "output_type": "error",
     "traceback": [
      "syntax: extra token \"will\" after end of expression",
      ""
     ]
    }
   ],
   "source": [
    "We will replace it with better implementation of SentencePiece "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"She ▁had ▁the ▁job ▁she ▁had ▁planned ▁for ▁the ▁last ▁three ▁years.\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1] = replace(sample[1],\" \"=> \" ▁\")\n",
    "sample[2] = replace(sample[2],\" \"=> \" ▁\")\n",
    "sample[3] = replace(sample[3],\" \"=> \" ▁\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Can use WordPiece for Word Segmentation with ALBERT vocabulary as follow. \n",
    "working on the APIs for Wordpiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{String,1}:\n",
       " \"God ▁is ▁Great! ▁I ▁won ▁a ▁lottery.\"                                                                                                                                                      \n",
       " \"If ▁all ▁their ▁conversations ▁in ▁the ▁three ▁months ▁he ▁had ▁been ▁coming ▁to ▁the ▁diner ▁were ▁put ▁together, ▁it ▁was ▁doubtful ▁that ▁they ▁would ▁make ▁a ▁respectable ▁paragraph.\"\n",
       " \"She ▁had ▁the ▁job ▁she ▁had ▁planned ▁for ▁the ▁last ▁three ▁years.\"                                                                                                                      "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Transformers.BidirectionalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Array{String,1},1}:\n",
       " [\"god\", \"▁is\", \"▁great\", \"!\", \"▁i\", \"▁won\", \"▁a\", \"▁lottery\", \".\"]                                                                                                                                 \n",
       " [\"if\", \"▁all\", \"▁their\", \"▁conversations\", \"▁in\", \"▁the\", \"▁three\", \"▁months\", \"▁he\", \"▁had\"  …  \"▁was\", \"▁doubtful\", \"▁that\", \"▁they\", \"▁would\", \"▁make\", \"▁a\", \"▁respectable\", \"▁paragraph\", \".\"]\n",
       " [\"she\", \"▁had\", \"▁the\", \"▁job\", \"▁she\", \"▁had\", \"▁planned\", \"▁for\", \"▁the\", \"▁last\", \"▁three\", \"▁years\", \".\"]                                                                                      "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenise.(sample, Val(true))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{SubString{String},1}:\n",
       " \"<pad>\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = readlines(\"/home/iamtejas/Downloads/albert_xlarge_v1/albert_xlarge/30k-clean.vocab\")\n",
    "vocabnew = split.(vocab , \"\\t\")\n",
    "vo = [vocabnew[1][1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need Score of SentencePiece unigram model for Wordpiece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30001-element Array{String,1}:\n",
       " \"<pad>\"        \n",
       " \"<pad>\"        \n",
       " \"<unk>\"        \n",
       " \"[CLS]\"        \n",
       " \"[SEP]\"        \n",
       " \"[MASK]\"       \n",
       " \"(\"            \n",
       " \")\"            \n",
       " \"\\\"\"           \n",
       " \"-\"            \n",
       " \".\"            \n",
       " \"–\"            \n",
       " \"£\"            \n",
       " ⋮              \n",
       " \"▁predation\"   \n",
       " \"▁saviour\"     \n",
       " \"▁archivist\"   \n",
       " \"▁obverse\"     \n",
       " \"error\"        \n",
       " \"▁tyrion\"      \n",
       " \"▁addictive\"   \n",
       " \"▁veneto\"      \n",
       " \"▁colloquial\"  \n",
       " \"agog\"         \n",
       " \"▁deficiencies\"\n",
       " \"▁eloquent\"    "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in 1:30000\n",
    "    vocab1 = vocabnew[i][1]\n",
    "    push!(vo,vocab1)\n",
    "end\n",
    "vocab1 = convert(Array{String,1},vo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000-element Array{String,1}:\n",
       " \"<pad>\"        \n",
       " \"<unk>\"        \n",
       " \"[CLS]\"        \n",
       " \"[SEP]\"        \n",
       " \"[MASK]\"       \n",
       " \"(\"            \n",
       " \")\"            \n",
       " \"\\\"\"           \n",
       " \"-\"            \n",
       " \".\"            \n",
       " \"–\"            \n",
       " \"£\"            \n",
       " \"€\"            \n",
       " ⋮              \n",
       " \"▁predation\"   \n",
       " \"▁saviour\"     \n",
       " \"▁archivist\"   \n",
       " \"▁obverse\"     \n",
       " \"error\"        \n",
       " \"▁tyrion\"      \n",
       " \"▁addictive\"   \n",
       " \"▁veneto\"      \n",
       " \"▁colloquial\"  \n",
       " \"agog\"         \n",
       " \"▁deficiencies\"\n",
       " \"▁eloquent\"    "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab1 = vocab1[2:30001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordPiece(vocab_size=30000, unk=<unk>, max_char=200)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp = WordPiece(vocab1,\"<unk>\"; max_char = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Array{String,1},1}:\n",
       " [\"god\", \"▁is\", \"▁great\", \"!\", \"▁i\", \"▁won\", \"▁a\", \"▁lottery\", \".\"]                                                                                                                                 \n",
       " [\"if\", \"▁all\", \"▁their\", \"▁conversations\", \"▁in\", \"▁the\", \"▁three\", \"▁months\", \"▁he\", \"▁had\"  …  \"▁was\", \"▁doubtful\", \"▁that\", \"▁they\", \"▁would\", \"▁make\", \"▁a\", \"▁respectable\", \"▁paragraph\", \".\"]\n",
       " [\"she\", \"▁had\", \"▁the\", \"▁job\", \"▁she\", \"▁had\", \"▁planned\", \"▁for\", \"▁the\", \"▁last\", \"▁three\", \"▁years\", \".\"]                                                                                      "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokensnew = wp.(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{String,1}:\n",
       " \"God ▁is ▁Great! ▁I ▁won ▁a ▁lottery.\"                                                                                                                                                      \n",
       " \"If ▁all ▁their ▁conversations ▁in ▁the ▁three ▁months ▁he ▁had ▁been ▁coming ▁to ▁the ▁diner ▁were ▁put ▁together, ▁it ▁was ▁doubtful ▁that ▁they ▁would ▁make ▁a ▁respectable ▁paragraph.\"\n",
       " \"She ▁had ▁the ▁job ▁she ▁had ▁planned ▁for ▁the ▁last ▁three ▁years.\"                                                                                                                      "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocabulary(30000, unk=<unk>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabalbert = Transformers.Basic.Vocabulary(wp.vocab,wp.vocab[wp.unk_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we have indices form sample tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30×3 Array{Int64,2}:\n",
       "  5475    822  1080\n",
       "    26     66    42\n",
       "   375     67    15\n",
       "   188  13528  1206\n",
       "    32     20    40\n",
       "   231     15    42\n",
       "    22    133  2036\n",
       " 17519    819    27\n",
       "    10     25    15\n",
       "     2     42   237\n",
       "     2     75   133\n",
       "     2    881   123\n",
       "     2     21    10\n",
       "     ⋮             \n",
       "     2     16     2\n",
       "     2     33     2\n",
       "     2     24     2\n",
       "     2  22569     2\n",
       "     2     31     2\n",
       "     2     60     2\n",
       "     2     84     2\n",
       "     2    234     2\n",
       "     2     22     2\n",
       "     2  22740     2\n",
       "     2  20600     2\n",
       "     2     10     2"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = vocabalbert(tokensnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 3)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we only take one sentence as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30×3 Array{Int64,2}:\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " ⋮      \n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_indices = ones(Int, size(indices)...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to turn those indices into embeddings.These can be done by loading embedding in Embed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SENTENCEPIECE\n",
    "now i will show how to use Sentencepiece  \n",
    "I am working on julia implementation of SentencePiece but for now we can use PyCall and also python Wrapper is under development\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment setup\n",
    "You can install Python binary package of SentencePiece with.\n",
    "\n",
    "> % pip install sentencepiece\n",
    "\n",
    "For detail visit - [Sentencpiece](https://github.com/google/sentencepiece)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using PyCall\n",
    "SentencePiece = pyimport(\"sentencepiece\")\n",
    "sp = SentencePiece.SentencePieceProcessor()\n",
    "sp.Load(\"/home/iamtejas/Downloads/albert_xlarge_v1/albert_xlarge/30k-clean.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avaliable APIs\n",
    "> sp.EncodeAsPieces\n",
    "\n",
    "> sp.EncodeAsIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{String,1}:\n",
       " \"God is Great! I won a lottery.\"                                                                                                                                 \n",
       " \"If all their conversations in the three months he had been coming to the diner were put together, it was doubtful that they would make a respectable paragraph.\"\n",
       " \"She had the job she had planned for the last three years.\"                                                                                                      "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample4 = \"God is Great! I won a lottery.\"\n",
    "sample5 = \"If all their conversations in the three months he had been coming to the diner were put together, it was doubtful that they would make a respectable paragraph.\"\n",
    "sample6 = \"She had the job she had planned for the last three years.\"\n",
    "sampleforsp = [sample4,sample5,sample6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15-element Array{String,1}:\n",
       " \"▁\"       \n",
       " \"G\"       \n",
       " \"od\"      \n",
       " \"▁is\"     \n",
       " \"▁\"       \n",
       " \"G\"       \n",
       " \"re\"      \n",
       " \"at\"      \n",
       " \"!\"       \n",
       " \"▁\"       \n",
       " \"I\"       \n",
       " \"▁won\"    \n",
       " \"▁a\"      \n",
       " \"▁lottery\"\n",
       " \".\"       "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.EncodeAsPieces(sampleforsp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15-element Array{Int64,1}:\n",
       "    13\n",
       "     1\n",
       "  5648\n",
       "    25\n",
       "    13\n",
       "     1\n",
       "    99\n",
       "   721\n",
       "   187\n",
       "    13\n",
       "     1\n",
       "   230\n",
       "    21\n",
       " 17518\n",
       "     9"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E1 = sp.EncodeAsIds(sampleforsp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32-element Array{Int64,1}:\n",
       "    13\n",
       "     1\n",
       "   410\n",
       "    65\n",
       "    66\n",
       " 13527\n",
       "    19\n",
       "    14\n",
       "   132\n",
       "   818\n",
       "    24\n",
       "    41\n",
       "    74\n",
       "     ⋮\n",
       "    15\n",
       "    32\n",
       "    23\n",
       " 22568\n",
       "    30\n",
       "    59\n",
       "    83\n",
       "   233\n",
       "    21\n",
       " 22739\n",
       " 20599\n",
       "     9"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E2 = sp.EncodeAsIds(sampleforsp[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15-element Array{Int64,1}:\n",
       "   13\n",
       "    1\n",
       "  438\n",
       "   41\n",
       "   14\n",
       " 1205\n",
       "   39\n",
       "   41\n",
       " 2035\n",
       "   26\n",
       "   14\n",
       "  236\n",
       "  132\n",
       "  122\n",
       "    9"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E3 = sp.EncodeAsIds(sampleforsp[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we are going to feed all the three sentence in the model we need to add pad token for all the input we for now we can to do it manually later will be automatically can be provided\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: convertVector not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: convertVector not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[81]:1"
     ]
    }
   ],
   "source": [
    "E = convertVector[E1,E2,E3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30×3 Array{Int64,2}:\n",
       "  5475    822  1080\n",
       "    26     66    42\n",
       "   375     67    15\n",
       "   188  13528  1206\n",
       "    32     20    40\n",
       "   231     15    42\n",
       "    22    133  2036\n",
       " 17519    819    27\n",
       "    10     25    15\n",
       "     2     42   237\n",
       "     2     75   133\n",
       "     2    881   123\n",
       "     2     21    10\n",
       "     ⋮             \n",
       "     2     16     2\n",
       "     2     33     2\n",
       "     2     24     2\n",
       "     2  22569     2\n",
       "     2     31     2\n",
       "     2     60     2\n",
       "     2     84     2\n",
       "     2    234     2\n",
       "     2     22     2\n",
       "     2  22740     2\n",
       "     2  20600     2\n",
       "     2     10     2"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30×3 Array{Int64,2}:\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " ⋮      \n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_idx = ones(Int, size(indices)...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to use released weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Transformers.Basic\n",
    "using Flux\n",
    "using Flux: loadparams!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BSON: @save, @load\n",
    "@load \"/home/iamtejas/Downloads/albert_base_v1.bson.tfbson\" config weights vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Symbol,Any} with 0 entries"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_emb = Embed(\n",
    "    config[\"embedding_size\"],\n",
    "    config[\"vocab_size\"]\n",
    "  )\n",
    "\n",
    "  seg_emb = Embed(\n",
    "    config[\"embedding_size\"],\n",
    "    config[\"type_vocab_size\"]\n",
    "  )\n",
    "\n",
    "  posi_emb = PositionEmbedding(\n",
    "    config[\"embedding_size\"],\n",
    "    config[\"max_position_embeddings\"];\n",
    "    trainable = true\n",
    "  )\n",
    "\n",
    " emb_post = Positionwise(\n",
    "    LayerNorm(\n",
    "      config[\"embedding_size\"]\n",
    "    ),\n",
    "       # Dropout(\n",
    "       #     config[\"hidden_dropout_prob\"]\n",
    "       # ) there is some problem in loading dropout\n",
    "  )\n",
    "\n",
    "embedding = Dict{Symbol, Any}()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Set([\"bert/embeddings/token_type_embeddings\", \"bert/embeddings/LayerNorm/gamma\", \"bert/embeddings/word_embeddings\", \"bert/embeddings/LayerNorm/beta\", \"bert/embeddings/position_embeddings\"])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vnames = keys(weights)\n",
    "embeddings_weights = filter(name->occursin(\"embeddings\", name), vnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    " for k ∈ embeddings_weights\n",
    "        if occursin(\"LayerNorm/gamma\", k)\n",
    "            loadparams!(emb_post[1].diag.α', [weights[k]]) #there is some problem with loading\n",
    "            embedding[:postprocessor] = emb_post\n",
    "        elseif occursin(\"LayerNorm/beta\", k)\n",
    "            loadparams!(emb_post[1].diag.β', [weights[k]])\n",
    "        elseif occursin(\"word_embeddings\", k)\n",
    "            loadparams!(tok_emb.embedding, [weights[k]])\n",
    "            embedding[:tok] = tok_emb\n",
    "        elseif occursin(\"position_embeddings\", k)\n",
    "            loadparams!(posi_emb.embedding, [weights[k]])\n",
    "            embedding[:pe] = posi_emb\n",
    "        elseif occursin(\"token_type_embeddings\", k)\n",
    "             loadparams!(seg_emb.embedding, [weights[k]])\n",
    "            embedding[:segment] = seg_emb\n",
    "        else\n",
    "            @warn \"unknown variable: $k\"\n",
    "        end\n",
    "    end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompositeEmbedding(tok = Embed(128), segment = Embed(128), pe = PositionEmbedding(128, max_len=512), postprocessor = Positionwise(LayerNorm(128)))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " albertembed = CompositeEmbedding(;embedding...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128×30×3 Array{Float32,3}:\n",
       "[:, :, 1] =\n",
       " -0.591439   1.13699    -0.025718   …   0.831079   0.704506    0.440932 \n",
       " -0.799194   0.0305692  -0.438631      -0.12549    0.0807379   0.121085 \n",
       "  2.17368   -0.469415    0.284542      -0.720768  -0.75624    -0.661375 \n",
       "  2.52347    1.03309     0.92254        0.410215   0.167004    0.0673373\n",
       " -0.176611  -1.4789     -0.681304       1.38727    1.40006     1.2112   \n",
       "  1.16329    1.35542     0.611702   …   1.28013    1.36642     1.50305  \n",
       " -1.98747   -1.66523     0.601882       2.24999    2.35954     2.23884  \n",
       " -0.310094   2.92642     0.415302       2.34161    2.22562     2.20565  \n",
       " -1.37891   -1.38538     0.118553       0.368      0.174424    0.377196 \n",
       "  1.39736   -0.675551   -0.398878       1.09734    1.24373     0.998397 \n",
       " -1.81548   -0.950838   -0.368885   …  -1.78001   -2.07668    -2.44817  \n",
       "  0.018042  -1.26416    -0.397725       1.02105    1.06319     1.03057  \n",
       " -1.21238    0.840682    0.598232      -1.14324   -1.12482    -1.15721  \n",
       "  ⋮                                 ⋱                                   \n",
       "  0.332487  -0.346085    2.04886       -0.558341  -0.442301   -0.5992   \n",
       "  1.01988    1.71199    -1.11575        0.845664   0.71135     0.288664 \n",
       " -0.377469   1.67638    -0.226781      -0.17799   -0.131086    0.125353 \n",
       " -1.94455    1.21288    -0.835447      -0.170818  -0.201496   -0.206867 \n",
       "  1.2508    -0.342576    2.3407     …  -0.402499  -0.625138   -0.711792 \n",
       "  1.23233   -0.763257    1.12458       -1.72298   -1.56975    -1.30608  \n",
       "  0.69318    0.382603    1.00294        0.909765   0.466593    0.0447997\n",
       "  3.8956    -0.925055    0.848945      -1.53776   -1.69152    -1.81041  \n",
       " -0.980867   0.650667    0.0801426      1.88424    1.70571     1.38184  \n",
       "  0.101726  -0.575587   -2.67617    …  -1.53183   -1.70644    -1.7706   \n",
       " -0.545019  -0.850822    0.905785      -1.79453   -2.05027    -2.0615   \n",
       " -0.156842   0.235855    2.00041       -0.859217  -1.1419     -1.46613  \n",
       "\n",
       "[:, :, 2] =\n",
       " -1.31858     0.0216667   1.74342   …   1.90131     0.720707  -0.307627  \n",
       " -2.22677    -0.122591   -2.22693      -1.1345     -0.767863   0.462022  \n",
       "  2.69618     0.0777337  -0.344445     -1.16416     0.910052  -2.48972   \n",
       "  2.44241    -1.49599    -1.09075      -1.42148    -1.39399   -0.960671  \n",
       " -1.70116    -2.96672     0.436728     -0.62373     1.00691    0.786272  \n",
       "  1.28972     0.369043   -0.161703  …  -0.0358379   0.881378   0.296311  \n",
       " -1.52076    -1.55033     1.28657       1.76875    -2.44636    0.438425  \n",
       "  2.04544     1.67637     0.257624     -0.92607    -0.522448   0.00572776\n",
       " -1.7461     -0.828101    0.20311      -1.67184    -2.20342   -1.10163   \n",
       "  2.37937     2.12046    -0.615439      1.45318     1.78176    1.17987   \n",
       " -2.23239    -0.975555    1.34697   …   1.20792     2.70698   -1.48264   \n",
       " -1.94884    -1.10907    -1.41559       0.881912    1.55691    1.83641   \n",
       " -0.739032   -1.60558    -0.397427      0.936388   -0.149812   2.66024   \n",
       "  ⋮                                 ⋱                                    \n",
       " -0.0568212  -1.96264    -2.33389      -0.300426   -0.91019    1.17974   \n",
       " -0.494863   -1.20462     1.08417       1.71642    -0.358318  -0.629219  \n",
       "  1.64246    -1.14133    -2.26024       0.346835   -2.02283    0.185394  \n",
       " -0.868637   -0.169764   -1.15969       0.193283    0.552946  -3.35316   \n",
       "  0.545062   -0.549176    0.540946  …   2.04645    -1.98427    1.35964   \n",
       "  1.92427    -0.889478    0.923516     -1.67208    -0.481406   0.0824193 \n",
       "  1.45381    -0.23133     0.526187      4.16982    -0.771515   0.413892  \n",
       "  1.90604     3.92324    -1.49929      -0.593308   -1.09789   -1.71067   \n",
       "  0.210802   -0.217772    1.44354       1.42685    -0.204244  -1.1955    \n",
       " -0.398544    1.1144     -1.20998   …  -0.811167    0.309671  -0.942952  \n",
       " -2.19551    -1.50174     1.23003      -0.639955   -0.626054  -2.86127   \n",
       "  0.401139    1.75107    -2.46705      -0.188901   -1.08455    1.41312   \n",
       "\n",
       "[:, :, 3] =\n",
       " -0.71346    -0.00312004   0.504392   …   0.831079   0.704506    0.440932 \n",
       " -1.92746     1.22748     -0.202011      -0.12549    0.0807379   0.121085 \n",
       "  0.243688    0.172815    -0.389571      -0.720768  -0.75624    -0.661375 \n",
       "  1.82114    -0.634088     0.884559       0.410215   0.167004    0.0673373\n",
       " -0.843611    1.85343     -2.01637        1.38727    1.40006     1.2112   \n",
       " -0.555372   -2.34414      0.980694   …   1.28013    1.36642     1.50305  \n",
       "  0.484204   -1.38045     -2.61899        2.24999    2.35954     2.23884  \n",
       "  1.41576     1.65953      0.0947241      2.34161    2.22562     2.20565  \n",
       " -0.26147    -0.025632     0.865253       0.368      0.174424    0.377196 \n",
       "  1.76851     2.69362     -0.233422       1.09734    1.24373     0.998397 \n",
       " -0.236484    1.67005      1.16324    …  -1.78001   -2.07668    -2.44817  \n",
       " -0.51392     0.766912     0.321797       1.02105    1.06319     1.03057  \n",
       " -1.58014     1.8389      -1.5822        -1.14324   -1.12482    -1.15721  \n",
       "  ⋮                                   ⋱                                   \n",
       "  0.170868    0.0334274   -1.51682       -0.558341  -0.442301   -0.5992   \n",
       " -1.07614     1.29878     -0.119166       0.845664   0.71135     0.288664 \n",
       "  0.254605   -0.382393     0.0796242     -0.17799   -0.131086    0.125353 \n",
       " -1.88207     1.5074      -0.682749      -0.170818  -0.201496   -0.206867 \n",
       "  1.11378     0.775584     2.64385    …  -0.402499  -0.625138   -0.711792 \n",
       "  0.0975123  -1.29187      0.0304766     -1.72298   -1.56975    -1.30608  \n",
       "  0.847709    0.212261    -0.135249       0.909765   0.466593    0.0447997\n",
       "  1.58948     1.39345      0.0182576     -1.53776   -1.69152    -1.81041  \n",
       " -2.01145     0.78437     -0.0896198      1.88424    1.70571     1.38184  \n",
       " -3.35792    -2.49736     -1.734      …  -1.53183   -1.70644    -1.7706   \n",
       " -1.95333    -1.65065     -1.07429       -1.79453   -2.05027    -2.0615   \n",
       "  0.339032   -1.30472      0.162728      -0.859217  -1.1419     -1.46613  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_wordpiece = albertembed(tok=indices, segment=seg_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we have embedding for or input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.5",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
