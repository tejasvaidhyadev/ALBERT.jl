{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work in Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installing** dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are keeping for now all the file of ALBERT in transformers.jl (it will Ultimately be added in TextAnalysis.jl with Transformers.jl as its dependency) we are using tokenize and Wordpiece(sentencepiece) from Albert which is based on Transformers.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are taking 3 sentence as sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{String,1}:\n",
       " \"God is Great! I won a lottery.\"                                                                                                                                 \n",
       " \"If all their conversations in the three months he had been coming to the diner were put together, it was doubtful that they would make a respectable paragraph.\"\n",
       " \"She had the job she had planned for the last three years.\"                                                                                                      "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1 = \"God is Great! I won a lottery.\"\n",
    "sample2 = \"If all their conversations in the three months he had been coming to the diner were put together, it was doubtful that they would make a respectable paragraph.\"\n",
    "sample3 = \"She had the job she had planned for the last three years.\"\n",
    "sample = [sample1,sample2,sample3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALBERT uses SentencePiece for word segmentation and Sentence segementation.\n",
    "Here we are going to provide both Sentencepiece as well as Wordpiece (using Vocab file of ALBERT) for ALBERT with the former as defalut one.\n",
    "We first demonstrate WordPiece\n",
    "To use word piece we need to make following changes as below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we first replace \" \"(space) with ' _' [space(U+2581)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decode split sentence. SentencePiece replace space with U+2581 and Store word in .vocab file with U+2581 as prefix .To tackle this we are using little trick as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "syntax: extra token \"will\" after end of expression",
     "output_type": "error",
     "traceback": [
      "syntax: extra token \"will\" after end of expression",
      ""
     ]
    }
   ],
   "source": [
    "We will replace it with better implementation of SentencePiece "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"She ▁had ▁the ▁job ▁she ▁had ▁planned ▁for ▁the ▁last ▁three ▁years.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1] = replace(sample[1],\" \"=> \" ▁\")\n",
    "sample[2] = replace(sample[2],\" \"=> \" ▁\")\n",
    "sample[3] = replace(sample[3],\" \"=> \" ▁\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Can use WordPiece for Word Segmentation with ALBERT vocabulary as follow. \n",
    "working on the APIs for Wordpiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{String,1}:\n",
       " \"God ▁is ▁Great! ▁I ▁won ▁a ▁lottery.\"                                                                                                                                                      \n",
       " \"If ▁all ▁their ▁conversations ▁in ▁the ▁three ▁months ▁he ▁had ▁been ▁coming ▁to ▁the ▁diner ▁were ▁put ▁together, ▁it ▁was ▁doubtful ▁that ▁they ▁would ▁make ▁a ▁respectable ▁paragraph.\"\n",
       " \"She ▁had ▁the ▁job ▁she ▁had ▁planned ▁for ▁the ▁last ▁three ▁years.\"                                                                                                                      "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Transformers.BidirectionalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Array{String,1},1}:\n",
       " [\"god\", \"▁is\", \"▁great\", \"!\", \"▁i\", \"▁won\", \"▁a\", \"▁lottery\", \".\"]                                                                                                                                 \n",
       " [\"if\", \"▁all\", \"▁their\", \"▁conversations\", \"▁in\", \"▁the\", \"▁three\", \"▁months\", \"▁he\", \"▁had\"  …  \"▁was\", \"▁doubtful\", \"▁that\", \"▁they\", \"▁would\", \"▁make\", \"▁a\", \"▁respectable\", \"▁paragraph\", \".\"]\n",
       " [\"she\", \"▁had\", \"▁the\", \"▁job\", \"▁she\", \"▁had\", \"▁planned\", \"▁for\", \"▁the\", \"▁last\", \"▁three\", \"▁years\", \".\"]                                                                                      "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenise.(sample, Val(true))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{SubString{String},1}:\n",
       " \"<pad>\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = readlines(\"/home/iamtejas/Downloads/albert_xlarge_v1/albert_xlarge/30k-clean.vocab\")\n",
    "vocabnew = split.(vocab , \"\\t\")\n",
    "vo = [vocabnew[1][1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need Score of SentencePiece unigram model for Wordpiece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30001-element Array{String,1}:\n",
       " \"<pad>\"        \n",
       " \"<pad>\"        \n",
       " \"<unk>\"        \n",
       " \"[CLS]\"        \n",
       " \"[SEP]\"        \n",
       " \"[MASK]\"       \n",
       " \"(\"            \n",
       " \")\"            \n",
       " \"\\\"\"           \n",
       " \"-\"            \n",
       " \".\"            \n",
       " \"–\"            \n",
       " \"£\"            \n",
       " ⋮              \n",
       " \"▁predation\"   \n",
       " \"▁saviour\"     \n",
       " \"▁archivist\"   \n",
       " \"▁obverse\"     \n",
       " \"error\"        \n",
       " \"▁tyrion\"      \n",
       " \"▁addictive\"   \n",
       " \"▁veneto\"      \n",
       " \"▁colloquial\"  \n",
       " \"agog\"         \n",
       " \"▁deficiencies\"\n",
       " \"▁eloquent\"    "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in 1:30000\n",
    "    vocab1 = vocabnew[i][1]\n",
    "    push!(vo,vocab1)\n",
    "end\n",
    "vocab1 = convert(Array{String,1},vo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000-element Array{String,1}:\n",
       " \"<pad>\"        \n",
       " \"<unk>\"        \n",
       " \"[CLS]\"        \n",
       " \"[SEP]\"        \n",
       " \"[MASK]\"       \n",
       " \"(\"            \n",
       " \")\"            \n",
       " \"\\\"\"           \n",
       " \"-\"            \n",
       " \".\"            \n",
       " \"–\"            \n",
       " \"£\"            \n",
       " \"€\"            \n",
       " ⋮              \n",
       " \"▁predation\"   \n",
       " \"▁saviour\"     \n",
       " \"▁archivist\"   \n",
       " \"▁obverse\"     \n",
       " \"error\"        \n",
       " \"▁tyrion\"      \n",
       " \"▁addictive\"   \n",
       " \"▁veneto\"      \n",
       " \"▁colloquial\"  \n",
       " \"agog\"         \n",
       " \"▁deficiencies\"\n",
       " \"▁eloquent\"    "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab1 = vocab1[2:30001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordPiece(vocab_size=30000, unk=<unk>, max_char=200)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp = WordPiece(vocab1,\"<unk>\"; max_char = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Array{String,1},1}:\n",
       " [\"god\", \"▁is\", \"▁great\", \"!\", \"▁i\", \"▁won\", \"▁a\", \"▁lottery\", \".\"]                                                                                                                                 \n",
       " [\"if\", \"▁all\", \"▁their\", \"▁conversations\", \"▁in\", \"▁the\", \"▁three\", \"▁months\", \"▁he\", \"▁had\"  …  \"▁was\", \"▁doubtful\", \"▁that\", \"▁they\", \"▁would\", \"▁make\", \"▁a\", \"▁respectable\", \"▁paragraph\", \".\"]\n",
       " [\"she\", \"▁had\", \"▁the\", \"▁job\", \"▁she\", \"▁had\", \"▁planned\", \"▁for\", \"▁the\", \"▁last\", \"▁three\", \"▁years\", \".\"]                                                                                      "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokensnew = wp.(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{String,1}:\n",
       " \"God ▁is ▁Great! ▁I ▁won ▁a ▁lottery.\"                                                                                                                                                      \n",
       " \"If ▁all ▁their ▁conversations ▁in ▁the ▁three ▁months ▁he ▁had ▁been ▁coming ▁to ▁the ▁diner ▁were ▁put ▁together, ▁it ▁was ▁doubtful ▁that ▁they ▁would ▁make ▁a ▁respectable ▁paragraph.\"\n",
       " \"She ▁had ▁the ▁job ▁she ▁had ▁planned ▁for ▁the ▁last ▁three ▁years.\"                                                                                                                      "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocabulary(30000, unk=<unk>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabalbert = Transformers.Basic.Vocabulary(wp.vocab,wp.vocab[wp.unk_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we have indices form sample tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30×3 Array{Int64,2}:\n",
       "  5475    822  1080\n",
       "    26     66    42\n",
       "   375     67    15\n",
       "   188  13528  1206\n",
       "    32     20    40\n",
       "   231     15    42\n",
       "    22    133  2036\n",
       " 17519    819    27\n",
       "    10     25    15\n",
       "     2     42   237\n",
       "     2     75   133\n",
       "     2    881   123\n",
       "     2     21    10\n",
       "     ⋮             \n",
       "     2     16     2\n",
       "     2     33     2\n",
       "     2     24     2\n",
       "     2  22569     2\n",
       "     2     31     2\n",
       "     2     60     2\n",
       "     2     84     2\n",
       "     2    234     2\n",
       "     2     22     2\n",
       "     2  22740     2\n",
       "     2  20600     2\n",
       "     2     10     2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = vocabalbert(tokensnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we only take one sentence as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30×3 Array{Int64,2}:\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " ⋮      \n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_indices = ones(Int, size(indices)...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to turn those indices into embeddings.These can be done by loading embedding in Embed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SENTENCEPIECE\n",
    "now i will show how to use Sentencepiece  \n",
    "I am working on julia implementation of SentencePiece but for now we can use PyCall and also python Wrapper is under development\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment setup\n",
    "You can install Python binary package of SentencePiece with.\n",
    "\n",
    "> % pip install sentencepiece\n",
    "\n",
    "For detail visit - [Sentencpiece](https://github.com/google/sentencepiece)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using PyCall\n",
    "SentencePiece = pyimport(\"sentencepiece\")\n",
    "sp = SentencePiece.SentencePieceProcessor()\n",
    "sp.Load(\"/home/iamtejas/Downloads/albert_xlarge_v1/albert_xlarge/30k-clean.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avaliable APIs\n",
    "> sp.EncodeAsPieces\n",
    "\n",
    "> sp.EncodeAsIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{String,1}:\n",
       " \"God is Great! I won a lottery.\"                                                                                                                                 \n",
       " \"If all their conversations in the three months he had been coming to the diner were put together, it was doubtful that they would make a respectable paragraph.\"\n",
       " \"She had the job she had planned for the last three years.\"                                                                                                      "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample4 = \"God is Great! I won a lottery.\"\n",
    "sample5 = \"If all their conversations in the three months he had been coming to the diner were put together, it was doubtful that they would make a respectable paragraph.\"\n",
    "sample6 = \"She had the job she had planned for the last three years.\"\n",
    "sampleforsp = [sample4,sample5,sample6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15-element Array{String,1}:\n",
       " \"▁\"       \n",
       " \"G\"       \n",
       " \"od\"      \n",
       " \"▁is\"     \n",
       " \"▁\"       \n",
       " \"G\"       \n",
       " \"re\"      \n",
       " \"at\"      \n",
       " \"!\"       \n",
       " \"▁\"       \n",
       " \"I\"       \n",
       " \"▁won\"    \n",
       " \"▁a\"      \n",
       " \"▁lottery\"\n",
       " \".\"       "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.EncodeAsPieces(sampleforsp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15-element Array{Int64,1}:\n",
       "    13\n",
       "     1\n",
       "  5648\n",
       "    25\n",
       "    13\n",
       "     1\n",
       "    99\n",
       "   721\n",
       "   187\n",
       "    13\n",
       "     1\n",
       "   230\n",
       "    21\n",
       " 17518\n",
       "     9"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E1 = sp.EncodeAsIds(sampleforsp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32-element Array{Int64,1}:\n",
       "    13\n",
       "     1\n",
       "   410\n",
       "    65\n",
       "    66\n",
       " 13527\n",
       "    19\n",
       "    14\n",
       "   132\n",
       "   818\n",
       "    24\n",
       "    41\n",
       "    74\n",
       "     ⋮\n",
       "    15\n",
       "    32\n",
       "    23\n",
       " 22568\n",
       "    30\n",
       "    59\n",
       "    83\n",
       "   233\n",
       "    21\n",
       " 22739\n",
       " 20599\n",
       "     9"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E2 = sp.EncodeAsIds(sampleforsp[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15-element Array{Int64,1}:\n",
       "   13\n",
       "    1\n",
       "  438\n",
       "   41\n",
       "   14\n",
       " 1205\n",
       "   39\n",
       "   41\n",
       " 2035\n",
       "   26\n",
       "   14\n",
       "  236\n",
       "  132\n",
       "  122\n",
       "    9"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E3 = sp.EncodeAsIds(sampleforsp[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32-element Array{Int64,1}:\n",
       "   13\n",
       "    1\n",
       " 5648\n",
       "   25\n",
       "   13\n",
       "    1\n",
       "   99\n",
       "  721\n",
       "  187\n",
       "   13\n",
       "    1\n",
       "  230\n",
       "   21\n",
       "    ⋮\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E1 = [E1;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32-element Array{Int64,1}:\n",
       "   13\n",
       "    1\n",
       "  438\n",
       "   41\n",
       "   14\n",
       " 1205\n",
       "   39\n",
       "   41\n",
       " 2035\n",
       "   26\n",
       "   14\n",
       "  236\n",
       "  132\n",
       "    ⋮\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E3 = [E3;;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we are going to feed all the three sentence in the model we need to add pad token for all the input we for now we can to do it manually later will be automatically can be provided\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32×3 Array{Int64,2}:\n",
       "   13     13    13\n",
       "    1      1     1\n",
       " 5648    410   438\n",
       "   25     65    41\n",
       "   13     66    14\n",
       "    1  13527  1205\n",
       "   99     19    39\n",
       "  721     14    41\n",
       "  187    132  2035\n",
       "   13    818    26\n",
       "    1     24    14\n",
       "  230     41   236\n",
       "   21     74   132\n",
       "    ⋮             \n",
       "    0     15     0\n",
       "    0     32     0\n",
       "    0     23     0\n",
       "    0  22568     0\n",
       "    0     30     0\n",
       "    0     59     0\n",
       "    0     83     0\n",
       "    0    233     0\n",
       "    0     21     0\n",
       "    0  22739     0\n",
       "    0  20599     0\n",
       "    0      9     0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = [E1 E2 E3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30×3 Array{Int64,2}:\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " ⋮      \n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_idx = ones(Int, size(indices)...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to use released weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Transformers.Basic\n",
    "using Flux\n",
    "using Flux: loadparams!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using BSON: @save, @load\n",
    "@load \"/home/iamtejas/Downloads/albert_base_v1.bson.tfbson\" config weights vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Symbol,Any} with 0 entries"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_emb = Embed(\n",
    "    config[\"embedding_size\"],\n",
    "    config[\"vocab_size\"]\n",
    "  )\n",
    "\n",
    "  seg_emb = Embed(\n",
    "    config[\"embedding_size\"],\n",
    "    config[\"type_vocab_size\"]\n",
    "  )\n",
    "\n",
    "  posi_emb = PositionEmbedding(\n",
    "    config[\"embedding_size\"],\n",
    "    config[\"max_position_embeddings\"];\n",
    "    trainable = true\n",
    "  )\n",
    "\n",
    " emb_post = Positionwise(\n",
    "    LayerNorm(\n",
    "      config[\"embedding_size\"]\n",
    "    ),\n",
    "       # Dropout(\n",
    "       #     config[\"hidden_dropout_prob\"]\n",
    "       # ) there is some problem in loading dropout\n",
    "  )\n",
    "\n",
    "embedding = Dict{Symbol, Any}()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Set([\"bert/embeddings/token_type_embeddings\", \"bert/embeddings/LayerNorm/gamma\", \"bert/embeddings/word_embeddings\", \"bert/embeddings/LayerNorm/beta\", \"bert/embeddings/position_embeddings\"])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vnames = keys(weights)\n",
    "embeddings_weights = filter(name->occursin(\"embeddings\", name), vnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " for k ∈ embeddings_weights\n",
    "        if occursin(\"LayerNorm/gamma\", k)\n",
    "            loadparams!(emb_post[1].diag.α', [weights[k]]) #there is some problem with loading\n",
    "            embedding[:postprocessor] = emb_post\n",
    "        elseif occursin(\"LayerNorm/beta\", k)\n",
    "            loadparams!(emb_post[1].diag.β', [weights[k]])\n",
    "        elseif occursin(\"word_embeddings\", k)\n",
    "            loadparams!(tok_emb.embedding, [weights[k]])\n",
    "            embedding[:tok] = tok_emb\n",
    "        elseif occursin(\"position_embeddings\", k)\n",
    "            loadparams!(posi_emb.embedding, [weights[k]])\n",
    "            embedding[:pe] = posi_emb\n",
    "        elseif occursin(\"token_type_embeddings\", k)\n",
    "             loadparams!(seg_emb.embedding, [weights[k]])\n",
    "            embedding[:segment] = seg_emb\n",
    "        else\n",
    "            @warn \"unknown variable: $k\"\n",
    "        end\n",
    "    end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompositeEmbedding(tok = Embed(128), segment = Embed(128), pe = PositionEmbedding(128, max_len=512), postprocessor = Positionwise(LayerNorm(128)))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " albertembed = CompositeEmbedding(;embedding...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128×30×3 Array{Float32,3}:\n",
       "[:, :, 1] =\n",
       " -0.591439   1.13699    -0.025718   …   0.831079   0.704506    0.440932 \n",
       " -0.799194   0.0305692  -0.438631      -0.12549    0.0807379   0.121085 \n",
       "  2.17368   -0.469415    0.284542      -0.720768  -0.75624    -0.661375 \n",
       "  2.52347    1.03309     0.92254        0.410215   0.167004    0.0673373\n",
       " -0.176611  -1.4789     -0.681304       1.38727    1.40006     1.2112   \n",
       "  1.16329    1.35542     0.611702   …   1.28013    1.36642     1.50305  \n",
       " -1.98747   -1.66523     0.601882       2.24999    2.35954     2.23884  \n",
       " -0.310094   2.92642     0.415302       2.34161    2.22562     2.20565  \n",
       " -1.37891   -1.38538     0.118553       0.368      0.174424    0.377196 \n",
       "  1.39736   -0.675551   -0.398878       1.09734    1.24373     0.998397 \n",
       " -1.81548   -0.950838   -0.368885   …  -1.78001   -2.07668    -2.44817  \n",
       "  0.018042  -1.26416    -0.397725       1.02105    1.06319     1.03057  \n",
       " -1.21238    0.840682    0.598232      -1.14324   -1.12482    -1.15721  \n",
       "  ⋮                                 ⋱                                   \n",
       "  0.332487  -0.346085    2.04886       -0.558341  -0.442301   -0.5992   \n",
       "  1.01988    1.71199    -1.11575        0.845664   0.71135     0.288664 \n",
       " -0.377469   1.67638    -0.226781      -0.17799   -0.131086    0.125353 \n",
       " -1.94455    1.21288    -0.835447      -0.170818  -0.201496   -0.206867 \n",
       "  1.2508    -0.342576    2.3407     …  -0.402499  -0.625138   -0.711792 \n",
       "  1.23233   -0.763257    1.12458       -1.72298   -1.56975    -1.30608  \n",
       "  0.69318    0.382603    1.00294        0.909765   0.466593    0.0447997\n",
       "  3.8956    -0.925055    0.848945      -1.53776   -1.69152    -1.81041  \n",
       " -0.980867   0.650667    0.0801426      1.88424    1.70571     1.38184  \n",
       "  0.101726  -0.575587   -2.67617    …  -1.53183   -1.70644    -1.7706   \n",
       " -0.545019  -0.850822    0.905785      -1.79453   -2.05027    -2.0615   \n",
       " -0.156842   0.235855    2.00041       -0.859217  -1.1419     -1.46613  \n",
       "\n",
       "[:, :, 2] =\n",
       " -1.31858     0.0216667   1.74342   …   1.90131     0.720707  -0.307627  \n",
       " -2.22677    -0.122591   -2.22693      -1.1345     -0.767863   0.462022  \n",
       "  2.69618     0.0777337  -0.344445     -1.16416     0.910052  -2.48972   \n",
       "  2.44241    -1.49599    -1.09075      -1.42148    -1.39399   -0.960671  \n",
       " -1.70116    -2.96672     0.436728     -0.62373     1.00691    0.786272  \n",
       "  1.28972     0.369043   -0.161703  …  -0.0358379   0.881378   0.296311  \n",
       " -1.52076    -1.55033     1.28657       1.76875    -2.44636    0.438425  \n",
       "  2.04544     1.67637     0.257624     -0.92607    -0.522448   0.00572776\n",
       " -1.7461     -0.828101    0.20311      -1.67184    -2.20342   -1.10163   \n",
       "  2.37937     2.12046    -0.615439      1.45318     1.78176    1.17987   \n",
       " -2.23239    -0.975555    1.34697   …   1.20792     2.70698   -1.48264   \n",
       " -1.94884    -1.10907    -1.41559       0.881912    1.55691    1.83641   \n",
       " -0.739032   -1.60558    -0.397427      0.936388   -0.149812   2.66024   \n",
       "  ⋮                                 ⋱                                    \n",
       " -0.0568212  -1.96264    -2.33389      -0.300426   -0.91019    1.17974   \n",
       " -0.494863   -1.20462     1.08417       1.71642    -0.358318  -0.629219  \n",
       "  1.64246    -1.14133    -2.26024       0.346835   -2.02283    0.185394  \n",
       " -0.868637   -0.169764   -1.15969       0.193283    0.552946  -3.35316   \n",
       "  0.545062   -0.549176    0.540946  …   2.04645    -1.98427    1.35964   \n",
       "  1.92427    -0.889478    0.923516     -1.67208    -0.481406   0.0824193 \n",
       "  1.45381    -0.23133     0.526187      4.16982    -0.771515   0.413892  \n",
       "  1.90604     3.92324    -1.49929      -0.593308   -1.09789   -1.71067   \n",
       "  0.210802   -0.217772    1.44354       1.42685    -0.204244  -1.1955    \n",
       " -0.398544    1.1144     -1.20998   …  -0.811167    0.309671  -0.942952  \n",
       " -2.19551    -1.50174     1.23003      -0.639955   -0.626054  -2.86127   \n",
       "  0.401139    1.75107    -2.46705      -0.188901   -1.08455    1.41312   \n",
       "\n",
       "[:, :, 3] =\n",
       " -0.71346    -0.00312004   0.504392   …   0.831079   0.704506    0.440932 \n",
       " -1.92746     1.22748     -0.202011      -0.12549    0.0807379   0.121085 \n",
       "  0.243688    0.172815    -0.389571      -0.720768  -0.75624    -0.661375 \n",
       "  1.82114    -0.634088     0.884559       0.410215   0.167004    0.0673373\n",
       " -0.843611    1.85343     -2.01637        1.38727    1.40006     1.2112   \n",
       " -0.555372   -2.34414      0.980694   …   1.28013    1.36642     1.50305  \n",
       "  0.484204   -1.38045     -2.61899        2.24999    2.35954     2.23884  \n",
       "  1.41576     1.65953      0.0947241      2.34161    2.22562     2.20565  \n",
       " -0.26147    -0.025632     0.865253       0.368      0.174424    0.377196 \n",
       "  1.76851     2.69362     -0.233422       1.09734    1.24373     0.998397 \n",
       " -0.236484    1.67005      1.16324    …  -1.78001   -2.07668    -2.44817  \n",
       " -0.51392     0.766912     0.321797       1.02105    1.06319     1.03057  \n",
       " -1.58014     1.8389      -1.5822        -1.14324   -1.12482    -1.15721  \n",
       "  ⋮                                   ⋱                                   \n",
       "  0.170868    0.0334274   -1.51682       -0.558341  -0.442301   -0.5992   \n",
       " -1.07614     1.29878     -0.119166       0.845664   0.71135     0.288664 \n",
       "  0.254605   -0.382393     0.0796242     -0.17799   -0.131086    0.125353 \n",
       " -1.88207     1.5074      -0.682749      -0.170818  -0.201496   -0.206867 \n",
       "  1.11378     0.775584     2.64385    …  -0.402499  -0.625138   -0.711792 \n",
       "  0.0975123  -1.29187      0.0304766     -1.72298   -1.56975    -1.30608  \n",
       "  0.847709    0.212261    -0.135249       0.909765   0.466593    0.0447997\n",
       "  1.58948     1.39345      0.0182576     -1.53776   -1.69152    -1.81041  \n",
       " -2.01145     0.78437     -0.0896198      1.88424    1.70571     1.38184  \n",
       " -3.35792    -2.49736     -1.734      …  -1.53183   -1.70644    -1.7706   \n",
       " -1.95333    -1.65065     -1.07429       -1.79453   -2.05027    -2.0615   \n",
       "  0.339032   -1.30472      0.162728      -0.859217  -1.1419     -1.46613  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_wordpiece = albertembed(tok=indices, segment=seg_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we have embedding from wordPiece "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Embedding from Sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32×3 Array{Int64,2}:\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " ⋮      \n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_idx_sentencepiece = ones(Int, size(E)...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128×32×3 Array{Float32,3}:\n",
       "[:, :, 1] =\n",
       " -1.27957   -0.433449   -0.0945636  …  -0.49776    -1.01922     -1.46112  \n",
       " -0.57838   -0.980475   -1.04832       -0.267728   -0.358444    -0.669767 \n",
       "  2.79966   -0.82644    -0.925393       0.128052    0.245785     0.351939 \n",
       " -0.709518  -0.429736   -1.00555       -0.254376    0.00938835   0.53717  \n",
       " -1.07688   -0.63944     0.818117       1.52909     1.14141      0.994143 \n",
       "  1.06804   -0.364175   -2.47114    …   0.945338    0.898164     0.723984 \n",
       " -1.31039   -1.17958    -0.123138       1.88349     1.43553      0.911783 \n",
       "  0.742485  -1.6949     -2.15704       -1.04646    -1.0591      -0.997114 \n",
       "  0.160304  -1.65645    -0.76259       -1.20975    -0.545515     0.448567 \n",
       "  1.53976    0.762963   -1.84859        2.10263     1.4055       0.538642 \n",
       " -1.72798   -0.458754   -1.34792    …   0.173283   -0.328897    -0.631871 \n",
       " -1.43214    0.546417    0.393653       2.23232     1.99174      1.72529  \n",
       "  2.65905    1.69725     1.18012        0.418767    0.0581364   -0.241486 \n",
       "  ⋮                                 ⋱               ⋮                     \n",
       "  0.235514  -1.22331     1.59174        0.714515    0.038753    -0.737029 \n",
       "  1.42639    0.200966    1.45457        0.670807   -0.209884    -0.8551   \n",
       " -1.19416    0.0664598   0.51833       -0.13604     0.353518     0.915717 \n",
       " -0.274545  -4.19593    -0.795705      -0.762659   -0.817944    -0.7823   \n",
       " -0.677418   0.693932    1.42931    …   0.214676    0.206883     0.203643 \n",
       " -0.310262   0.66076     0.293377      -0.784182   -0.356642    -0.0128708\n",
       "  1.16589    0.471086    0.295495       0.0759943  -0.653717    -0.869485 \n",
       " -2.13941   -0.584051    1.25496       -1.15236    -1.24928     -1.23776  \n",
       "  0.172668   0.934219    1.05735        0.323621    0.022627    -0.261138 \n",
       " -1.25105    1.53808     0.872353   …  -0.895192   -0.983603    -1.01397  \n",
       " -2.01311   -1.98916    -0.408156      -1.57407    -1.29678     -0.894344 \n",
       " -2.59789   -0.793182   -0.743739      -0.529029   -0.615649    -0.415223 \n",
       "\n",
       "[:, :, 2] =\n",
       " -1.27957   -0.433449    0.0587588  …  -3.88182   -1.05311   -0.620954 \n",
       " -0.57838   -0.980475   -0.0501686     -2.36654   -1.95715   -1.32528  \n",
       "  2.79966   -0.82644     1.84798        0.312029   0.180432   2.76545  \n",
       " -0.709518  -0.429736   -2.00202        0.243687  -1.03182   -0.70123  \n",
       " -1.07688   -0.63944    -2.13573       -0.457594   0.943395   1.58199  \n",
       "  1.06804   -0.364175   -1.29151    …   1.01511    2.17193    0.290624 \n",
       " -1.31039   -1.17958    -1.12896       -0.464006   0.551407  -0.700679 \n",
       "  0.742485  -1.6949     -2.73227        0.728846  -0.785376  -1.45704  \n",
       "  0.160304  -1.65645    -0.247414      -0.862263  -2.10266   -0.0470197\n",
       "  1.53976    0.762963   -1.79958        0.180442   1.95153    1.32913  \n",
       " -1.72798   -0.458754   -1.86924    …   0.519979  -1.47792   -2.04575  \n",
       " -1.43214    0.546417   -0.168444       1.14409    2.3977     0.450023 \n",
       "  2.65905    1.69725     0.421323       1.61373    2.06196   -2.77844  \n",
       "  ⋮                                 ⋱              ⋮                   \n",
       "  0.235514  -1.22331     0.47408        0.936106   0.163577  -1.0114   \n",
       "  1.42639    0.200966   -1.64713        0.777183  -1.65086    0.198584 \n",
       " -1.19416    0.0664598   1.0453         0.753156   1.29176    1.25277  \n",
       " -0.274545  -4.19593     0.656163      -1.69731   -0.621652  -1.29391  \n",
       " -0.677418   0.693932   -0.665332   …  -2.0898    -0.674343  -2.30267  \n",
       " -0.310262   0.66076    -0.01856        1.70964    2.19389    0.412881 \n",
       "  1.16589    0.471086   -0.389558      -1.45934   -1.63116   -1.27572  \n",
       " -2.13941   -0.584051    0.158215      -1.83854    0.833213  -1.93776  \n",
       "  0.172668   0.934219    1.66496       -0.450299  -0.331144  -1.81304  \n",
       " -1.25105    1.53808    -1.75199    …   0.101837   0.860925  -2.61297  \n",
       " -2.01311   -1.98916    -1.92456       -2.02094   -1.65074   -1.32756  \n",
       " -2.59789   -0.793182   -2.98306       -1.39067   -2.03007   -0.798481 \n",
       "\n",
       "[:, :, 3] =\n",
       " -1.27957   -0.433449   -0.124065   …  -0.49776    -1.01922     -1.46112  \n",
       " -0.57838   -0.980475    0.115883      -0.267728   -0.358444    -0.669767 \n",
       "  2.79966   -0.82644     0.0407071      0.128052    0.245785     0.351939 \n",
       " -0.709518  -0.429736    1.57828       -0.254376    0.00938835   0.53717  \n",
       " -1.07688   -0.63944     1.31921        1.52909     1.14141      0.994143 \n",
       "  1.06804   -0.364175   -1.19262    …   0.945338    0.898164     0.723984 \n",
       " -1.31039   -1.17958    -1.31321        1.88349     1.43553      0.911783 \n",
       "  0.742485  -1.6949     -0.70032       -1.04646    -1.0591      -0.997114 \n",
       "  0.160304  -1.65645    -0.533716      -1.20975    -0.545515     0.448567 \n",
       "  1.53976    0.762963    1.48478        2.10263     1.4055       0.538642 \n",
       " -1.72798   -0.458754   -1.84226    …   0.173283   -0.328897    -0.631871 \n",
       " -1.43214    0.546417    0.0923106      2.23232     1.99174      1.72529  \n",
       "  2.65905    1.69725     1.04722        0.418767    0.0581364   -0.241486 \n",
       "  ⋮                                 ⋱               ⋮                     \n",
       "  0.235514  -1.22331    -0.491815       0.714515    0.038753    -0.737029 \n",
       "  1.42639    0.200966    0.432006       0.670807   -0.209884    -0.8551   \n",
       " -1.19416    0.0664598   0.833823      -0.13604     0.353518     0.915717 \n",
       " -0.274545  -4.19593     1.5381        -0.762659   -0.817944    -0.7823   \n",
       " -0.677418   0.693932    2.63043    …   0.214676    0.206883     0.203643 \n",
       " -0.310262   0.66076    -1.56424       -0.784182   -0.356642    -0.0128708\n",
       "  1.16589    0.471086   -0.333791       0.0759943  -0.653717    -0.869485 \n",
       " -2.13941   -0.584051    0.88515       -1.15236    -1.24928     -1.23776  \n",
       "  0.172668   0.934219   -2.99496        0.323621    0.022627    -0.261138 \n",
       " -1.25105    1.53808     0.497613   …  -0.895192   -0.983603    -1.01397  \n",
       " -2.01311   -1.98916     1.66425       -1.57407    -1.29678     -0.894344 \n",
       " -2.59789   -0.793182    0.543231      -0.529029   -0.615649    -0.415223 "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_wordpiece = albertembed(tok=E, segment=seg_idx_sentencepiece)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above we have embedding from SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALBERT TRANSFORMER WRAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Transformers\n",
    "using Flux: @functor\n",
    "using MacroTools: @forward\n",
    "\n",
    "using Transformers.Basic\n",
    "using Transformers.Basic: AbstractTransformer\n",
    "using Transformers.Stacks\n",
    "\n",
    "struct albert <: AbstractTransformer\n",
    "  ts::Stack\n",
    "end\n",
    "\n",
    "@functor albert\n",
    "\n",
    "@forward albert.ts Base.getindex, Base.length\n",
    "\n",
    "\n",
    "function albert(size::Int, head::Int, ps::Int, layer::Int;\n",
    "              act = gelu, attn_pdrop = 0)\n",
    "  rem(size,  head) != 0 && error(\"size not divisible by head\")\n",
    "  albert(size, head, div(size, head), ps, layer; act=act, attn_pdrop=attn_pdrop)\n",
    "end\n",
    "\n",
    "function albert(size::Int, head::Int, hs::Int, ps::Int, layer::Int; act = gelu, attn_pdrop = 0)\n",
    "  albert(\n",
    "    Stack(\n",
    "      @nntopo_str(\"((x, m) => x':(x, m)) => $layer\"),\n",
    "      [\n",
    "        Transformer(size, head, hs, ps; future=true, act=act, pdrop=attn_pdrop)\n",
    "        for i = 1:layer\n",
    "      ]...\n",
    "    )\n",
    "         )\n",
    "end\n",
    "\n",
    "function (al::albert)(x::T, mask=nothing; all::Bool=false) where T\n",
    "  e = x\n",
    "\n",
    "  if mask === nothing\n",
    "    t, ts = al.ts(e, nothing)\n",
    "  else\n",
    "    t, ts = al.ts(e, getmask(mask, mask))\n",
    "  end\n",
    "\n",
    "  if all\n",
    "    if mask !== nothing\n",
    "      ts = map(ts) do ti\n",
    "        ti .* mask\n",
    "      end\n",
    "    end\n",
    "    ts[end], ts\n",
    "  else\n",
    "    t = mask === nothing ? t : t .* mask\n",
    "    t\n",
    "  end\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    masklmloss(embed::Embed{T}, transform,\n",
    "               t::AbstractArray{T, N}, posis::AbstractArray{Tuple{Int,Int}}, labels) where {T,N}\n",
    "    masklmloss(embed::Embed{T}, transform, output_bias,\n",
    "               t::AbstractArray{T, N}, posis::AbstractArray{Tuple{Int,Int}}, labels) where {T,N}\n",
    "\n",
    "helper function for computing the maks language modeling loss.\n",
    "Performance `transform(x) .+ output_bias` where `x` is the mask specified by\n",
    "`posis`, then compute the similarity with `embed.embedding` and crossentropy between true `labels`.\n",
    "\"\"\"\n",
    "function masklmloss(embed::Embed{T}, transform, t::AbstractArray{T, N}, posis::AbstractArray{Tuple{Int,Int}}, labels) where {T,N}\n",
    "  masktok = gather(t, posis)\n",
    "  sim = logsoftmax(transpose(embed.embedding) * transform(masktok))\n",
    "  return logcrossentropy(labels, sim)\n",
    "end\n",
    "\n",
    "function masklmloss(embed::Embed{T}, transform, output_bias, t::AbstractArray{T, N}, posis::AbstractArray{Tuple{Int,Int}}, labels) where {T,N}\n",
    "  masktok = gather(t, posis)\n",
    "  sim = logsoftmax(transpose(embed.embedding) * transform(masktok) .+ output_bias)\n",
    "  return logcrossentropy(labels, sim)\n",
    "end\n",
    "\n",
    "\n",
    "function Base.show(io::IO, al::albert)\n",
    "  hs = div(size(al.ts[1].mh.iqproj.W)[1], al.ts[1].mh.head)\n",
    "  h, ps = size(al.ts[1].pw.dout.W)\n",
    "\n",
    "  print(io, \"albert(\")\n",
    "  print(io, \"layers=$(length(al.ts)), \")\n",
    "  print(io, \"head=$(al.ts[1].mh.head), \")\n",
    "  print(io, \"head_size=$(hs), \")\n",
    "  print(io, \"pwffn_size=$(ps), \")\n",
    "  print(io, \"size=$(h))\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching albert()\nClosest candidates are:\n  albert(!Matched::Int64, !Matched::Int64, !Matched::Int64, !Matched::Int64; act, attn_pdrop) at In[87]:20\n  albert(!Matched::Int64, !Matched::Int64, !Matched::Int64, !Matched::Int64, !Matched::Int64; act, attn_pdrop) at In[87]:25\n  albert(!Matched::Stack) at In[87]:10\n  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching albert()\nClosest candidates are:\n  albert(!Matched::Int64, !Matched::Int64, !Matched::Int64, !Matched::Int64; act, attn_pdrop) at In[87]:20\n  albert(!Matched::Int64, !Matched::Int64, !Matched::Int64, !Matched::Int64, !Matched::Int64; act, attn_pdrop) at In[87]:25\n  albert(!Matched::Stack) at In[87]:10\n  ...",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[91]:1"
     ]
    }
   ],
   "source": [
    "albert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "load_albert_from_tfbson (generic function with 1 method)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function load_albert_from_tfbson(config, weights)\n",
    "    #init albert model possible component\n",
    "    albert = albert(\n",
    "        config[\"hidden_size\"],\n",
    "        config[\"num_attention_heads\"],\n",
    "        config[\"intermediate_size\"],\n",
    "        config[\"num_hidden_layers\"];\n",
    "        act = get_activation(config[\"hidden_act\"]),\n",
    "        embedding =config[\"embedding_size\"],\n",
    "        pdrop = config[\"hidden_dropout_prob\"],\n",
    "        attn_pdrop = config[\"attention_probs_dropout_prob\"]\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_activation (generic function with 1 method)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_activation(act_string)\n",
    "    if act_string == \"gelu\"\n",
    "        gelu\n",
    "    elseif act_string == \"relu\"\n",
    "        relu\n",
    "    elseif act_string == \"tanh\"\n",
    "        tanh\n",
    "    elseif act_string == \"linear\"\n",
    "        identity\n",
    "    else\n",
    "        throw(DomainError(act_string, \"activation support: linear, gelu, relu, tanh\"))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: albert not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: albert not defined",
      "",
      "Stacktrace:",
      " [1] load_albert_from_tfbson(::Dict{String,Any}, ::Dict{String,Array}) at ./In[92]:3",
      " [2] top-level scope at In[94]:1"
     ]
    }
   ],
   "source": [
    "albert_model = load_albert_from_tfbson(config, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.5",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
